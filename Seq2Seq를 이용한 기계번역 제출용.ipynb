{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc2pgM0w9Xa1"
      },
      "source": [
        "#### **Sequence to Sequence Learning with Neural Networks (NIPS 2014)** 실습\n",
        "* 본 코드는 기본적으로 **Seq2Seq** 논문의 내용을 따릅니다.\n",
        "    * 본 논문은 **딥러닝 기반의 자연어 처리** 기법의 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다.\n",
        "    * 2020년 기준 가장 뛰어난 번역 모델은 Seq2Seq가 아닌 **Transformer 기반의 모델**입니다.\n",
        "* 코드 실행 전에 **[런타임]** → **[런타임 유형 변경]** → 유형을 **GPU**로 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_UOI3Xm4U4M"
      },
      "source": [
        "#### **데이터 전처리(Preprocessing)**\n",
        "\n",
        "* **spaCy 라이브러리**: 문장의 토큰화(tokenization), 태깅(tagging) 등의 전처리 기능을 위한 라이브러리\n",
        "  * 영어(Engilsh)와 독일어(Deutsch) 전처리 모듈 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x8SEN31g34aX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s7u-Xt2c4WV8"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load(\"en_core_web_sm\") # 영어 토큰화(tokenization)\n",
        "spacy_de = spacy.load(\"de_core_news_sm\") # 독일어 토큰화(tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vnjcXO84aRa",
        "outputId": "09c0c624-c9d4-472d-9ad7-53609c5b6f8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: am\n",
            "인덱스 2: a\n",
            "인덱스 3: graduate\n",
            "인덱스 4: student\n",
            "인덱스 5: .\n"
          ]
        }
      ],
      "source": [
        "# 간단히 토큰화(tokenization) 기능 써보기\n",
        "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "    print(f\"인덱스 {i}: {token.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hskqq3f4-YT"
      },
      "source": [
        "* 영어(English) 및 독일어(Deutsch) **토큰화 함수** 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1EH0gEEb4iTj"
      },
      "outputs": [],
      "source": [
        "# 독일어(Deutsch) 문장을 토큰화한 뒤에 순서를 뒤집는 함수\n",
        "def tokenize_de(text):\n",
        "    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "# 영어(English) 문장을 토큰화 하는 함수\n",
        "def tokenize_en(text):\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-fAQN1O6_a9"
      },
      "source": [
        "* **필드(field)** 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시합니다.\n",
        "* 번역 목표\n",
        "    * 소스(SRC): 독일어\n",
        "    * 목표(TRG): 영어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MVsQFWVmazAM"
      },
      "outputs": [],
      "source": [
        "import torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3MN7tLQ8wzG"
      },
      "source": [
        "* 대표적인 영어-독어 번역 데이터셋인 **Multi30k**를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "vkAL7LBJdmzG",
        "outputId": "e1717f04-f4a6-4b91-ce3b-5985e42ed7aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchdata) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Installing collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.5.1 torchdata-0.4.1 urllib3-1.25.11\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "PaEW2EpY72jt"
      },
      "outputs": [],
      "source": [
        "import torchdata\n",
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "train_dataset = Multi30k(root='.data', split=('train'), language_pair=(\"de\", \"en\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "41tiyqEDeF8N"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "en = []\n",
        "de = []\n",
        "for label, line in train_dataset:\n",
        "    de+=[tokenize_de(label)]\n",
        "    en+=[tokenize_en(line)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJQmiqRXkn-4",
        "outputId": "351069ae-4d15-4aca-be05-60a847e936e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Two',\n",
              "  'young',\n",
              "  ',',\n",
              "  'White',\n",
              "  'males',\n",
              "  'are',\n",
              "  'outside',\n",
              "  'near',\n",
              "  'many',\n",
              "  'bushes',\n",
              "  '.'],\n",
              " ['Several',\n",
              "  'men',\n",
              "  'in',\n",
              "  'hard',\n",
              "  'hats',\n",
              "  'are',\n",
              "  'operating',\n",
              "  'a',\n",
              "  'giant',\n",
              "  'pulley',\n",
              "  'system',\n",
              "  '.']]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvPZWqQk9Ea1"
      },
      "source": [
        "* **필드(field)** 객체의 **build_vocab** 메서드를 이용해 영어와 독어의 단어 사전을 생성합니다.\n",
        "  * **최소 2번 이상** 등장한 단어만을 선택합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "9JNPXl-QVI6q"
      },
      "outputs": [],
      "source": [
        "sos_token='<sos>'\n",
        "eos_token='<eos>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "QUwwB9bMeTNG"
      },
      "outputs": [],
      "source": [
        "TRG=torchtext.vocab.build_vocab_from_iterator(en,min_freq=2,specials=['<unk>',sos_token,eos_token])\n",
        "SRC=torchtext.vocab.build_vocab_from_iterator(de,min_freq=2,specials=['<unk>',sos_token,eos_token])\n",
        "TRG.set_default_index(0)\n",
        "SRC.set_default_index(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crY-cQsygvyz",
        "outputId": "fc11d922-de2e-421a-b630-03fb7d541526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6190\n",
            "8013\n"
          ]
        }
      ],
      "source": [
        "print(len(TRG.vocab))\n",
        "print(len(SRC.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fqxBawjVdnG",
        "outputId": "c54e6150-ee0d-4c48-e00a-8d0831cf47dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SRC[sos_token]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZDT0KF1Vhou",
        "outputId": "2906d96e-548b-4694-cea6-d5665d06e84a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SRC[eos_token]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTST61ysbG-e",
        "outputId": "5fc51188-e2ec-4f8a-e8a1-09fcabe700cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SRC[\"adsafwe\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-B_3ZNMiPap",
        "outputId": "5a0c2281-70a2-46ba-e6ab-cb6da64afc80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'‘': 8012,\n",
              " 'üppigen': 8011,\n",
              " 'überwacht': 8009,\n",
              " 'überlegt': 8005,\n",
              " 'überfliegt': 8001,\n",
              " 'örtlicher': 8000,\n",
              " 'öffentliches': 7999,\n",
              " 'Überschlag': 7996,\n",
              " 'Überreste': 7995,\n",
              " 'Überhang': 7994,\n",
              " 'Äste': 7989,\n",
              " 'zwirbelt': 7986,\n",
              " 'zweispurigen': 7984,\n",
              " 'zusammenpassenden': 7982,\n",
              " 'zusammengebaut': 7980,\n",
              " 'zugedeckt': 7977,\n",
              " 'zufährt': 7976,\n",
              " 'zuerst': 7975,\n",
              " 'zotteliger': 7974,\n",
              " 'zerzaust': 7973,\n",
              " 'zerrissenen': 7972,\n",
              " 'zerlumpter': 7971,\n",
              " 'zerlegen': 7970,\n",
              " 'zerbrochenen': 7966,\n",
              " 'zerbrochene': 7965,\n",
              " 'zerbricht': 7964,\n",
              " 'zehn': 7961,\n",
              " 'you': 7959,\n",
              " 'wütend': 7958,\n",
              " 'wunderschön': 7954,\n",
              " 'wodurch': 7951,\n",
              " 'wirbeln': 7949,\n",
              " 'winterlichen': 7948,\n",
              " 'winkend': 7947,\n",
              " 'wichtiges': 7944,\n",
              " 'wetteifert': 7943,\n",
              " 'werdendem': 7942,\n",
              " 'wer': 7941,\n",
              " 'wenden': 7940,\n",
              " 'weißhaarige': 7938,\n",
              " 'weißene': 7937,\n",
              " 'weiß-rotes': 7936,\n",
              " 'weiß-braunem': 7935,\n",
              " 'weinroten': 7933,\n",
              " 'weiblich': 7932,\n",
              " 'wehrt': 7931,\n",
              " 'wegschaut': 7930,\n",
              " 'wartende': 7929,\n",
              " 'vorzubereiten': 7925,\n",
              " 'vornehmen': 7922,\n",
              " 'vordere': 7920,\n",
              " 'vorbeizukommen': 7919,\n",
              " 'vorbeiläuft': 7918,\n",
              " 'vorbeikommt': 7917,\n",
              " 'vorbeifahrenden': 7916,\n",
              " 'visiert': 7914,\n",
              " 'violette': 7913,\n",
              " 'vierte': 7912,\n",
              " 'vielfältige': 7910,\n",
              " 'vieler': 7909,\n",
              " 'verzweifelt': 7907,\n",
              " 'vertieft': 7902,\n",
              " 'versorgt': 7901,\n",
              " 'verschlungenen': 7896,\n",
              " 'verschiedenfarbiger': 7895,\n",
              " 'verregneten': 7893,\n",
              " 'verpasst': 7891,\n",
              " 'vermischt': 7890,\n",
              " 'verlegen': 7889,\n",
              " 'verheirateter': 7885,\n",
              " 'vergraben': 7883,\n",
              " 'vereinzelten': 7881,\n",
              " 'verborgen': 7880,\n",
              " 'verbeugen': 7878,\n",
              " 'unterstützen': 7876,\n",
              " 'unterirdischen': 7874,\n",
              " 'unglückliches': 7873,\n",
              " 'ungepflegten': 7871,\n",
              " 'umgekippt': 7865,\n",
              " 'umgedrehte': 7863,\n",
              " 'umgebene': 7862,\n",
              " 'umdreht': 7860,\n",
              " 'türkise': 7859,\n",
              " 'töpfert': 7858,\n",
              " 'tätowierten': 7857,\n",
              " 'trösten': 7854,\n",
              " 'trommeln': 7851,\n",
              " 'trockene': 7850,\n",
              " 'transparenten': 7847,\n",
              " 'verschwommenen': 7899,\n",
              " 'traditionellem': 7846,\n",
              " 'toten': 7843,\n",
              " 'tollt': 7842,\n",
              " 'to': 7841,\n",
              " 'tiefblauen': 7840,\n",
              " 'testet': 7838,\n",
              " 'teilnehmende': 7837,\n",
              " 'süßer': 7833,\n",
              " 'sumpfiges': 7832,\n",
              " 'sumpfigen': 7831,\n",
              " 'stürmt': 7830,\n",
              " 'stürmischem': 7829,\n",
              " 'stämmiger': 7828,\n",
              " 'struppiger': 7826,\n",
              " 'striegelt': 7825,\n",
              " 'stilles': 7820,\n",
              " 'stiert': 7819,\n",
              " 'sticht': 7818,\n",
              " 'steuern': 7817,\n",
              " 'stets': 7816,\n",
              " 'steiniges': 7815,\n",
              " 'steigenden': 7814,\n",
              " 'stattfindet': 7813,\n",
              " 'stattfindenden': 7812,\n",
              " 'starten': 7810,\n",
              " 'startbereit': 7809,\n",
              " 'starker': 7808,\n",
              " 'spät': 7805,\n",
              " 'spitzen': 7803,\n",
              " 'speist': 7798,\n",
              " 'speisen': 7797,\n",
              " 'sorgt': 7794,\n",
              " 'sonniger': 7793,\n",
              " 'sogar': 7792,\n",
              " 'singend': 7791,\n",
              " 'silbernem': 7790,\n",
              " 'silberfarbene': 7789,\n",
              " 'signiert': 7788,\n",
              " 'sichtbare': 7787,\n",
              " 'senkrechte': 7785,\n",
              " 'sendet': 7784,\n",
              " 'selbstgebauten': 7783,\n",
              " 'sehe': 7780,\n",
              " 'schürt': 7777,\n",
              " 'schön': 7776,\n",
              " 'schäbig': 7775,\n",
              " 'schwimmende': 7772,\n",
              " 'schubst': 7766,\n",
              " 'schrägen': 7765,\n",
              " 'schroffen': 7764,\n",
              " 'schreienden': 7763,\n",
              " 'schottischer': 7761,\n",
              " 'schnüffeln': 7760,\n",
              " 'schlittert': 7756,\n",
              " 'schlammiges': 7753,\n",
              " 'schlammigem': 7752,\n",
              " 'schirmen': 7750,\n",
              " 'scheucht': 7748,\n",
              " 'schert': 7747,\n",
              " 'schafft': 7746,\n",
              " 'sausen': 7745,\n",
              " 'sanft': 7743,\n",
              " 's': 7742,\n",
              " 'rötlichem': 7741,\n",
              " 'rund': 7739,\n",
              " 'rot-grünen': 7737,\n",
              " 'rot-goldenen': 7736,\n",
              " 'rot-gelben': 7734,\n",
              " 'rosa-grünen': 7732,\n",
              " 'rodelt': 7731,\n",
              " 'riss': 7729,\n",
              " 'reißen': 7723,\n",
              " 'regenbogenfarbigen': 7719,\n",
              " 'reckt': 7717,\n",
              " 'rechteckigen': 7716,\n",
              " 'reagiert': 7715,\n",
              " 'rausgeworfen': 7714,\n",
              " 'rauen': 7713,\n",
              " 'rasten': 7712,\n",
              " 'rasiertem': 7711,\n",
              " 'pusten': 7707,\n",
              " 'purpurroten': 7706,\n",
              " 'prächtige': 7704,\n",
              " 'professionell': 7702,\n",
              " 'pro': 7701,\n",
              " 'presst': 7700,\n",
              " 'praktizieren': 7698,\n",
              " 'plaudern': 7697,\n",
              " 'pinkfarbenes': 7695,\n",
              " 'schreibend': 7762,\n",
              " 'pink-weißem': 7694,\n",
              " 'patrouilliert': 7690,\n",
              " 'patriotischen': 7689,\n",
              " 'passender': 7687,\n",
              " 'passendem': 7686,\n",
              " 'passende': 7685,\n",
              " 'organgefarbenen': 7683,\n",
              " 'orange-weiß': 7681,\n",
              " 'orange-grauen': 7680,\n",
              " 'orange-blauen': 7679,\n",
              " 'operiert': 7678,\n",
              " 'nähen': 7677,\n",
              " 'nuckelt': 7674,\n",
              " 'normaler': 7673,\n",
              " 'niemand': 7672,\n",
              " 'neun': 7670,\n",
              " 'neugieriges': 7669,\n",
              " 'neugeborenen': 7668,\n",
              " 'neugeborene': 7667,\n",
              " 'neonfarbenen': 7665,\n",
              " 'nassgespritzt': 7664,\n",
              " 'nachgehen': 7662,\n",
              " 'nachdenklich': 7661,\n",
              " 'mürrisch': 7660,\n",
              " 'muslimischer': 7658,\n",
              " 'typischen': 7855,\n",
              " 'muslimische': 7657,\n",
              " 'musikalische': 7655,\n",
              " 'moosbedeckten': 7654,\n",
              " 'molliges': 7653,\n",
              " 'modischer': 7652,\n",
              " 'modernes': 7650,\n",
              " 'modelliert': 7648,\n",
              " 'mobile': 7647,\n",
              " 'mittlerem': 7646,\n",
              " 'mischen': 7644,\n",
              " 'militärischen': 7643,\n",
              " 'mexikanische': 7642,\n",
              " 'meine': 7637,\n",
              " 'mehrfarbiger': 7636,\n",
              " 'marineblauer': 7633,\n",
              " 'manövrieren': 7632,\n",
              " 'malerischen': 7631,\n",
              " 'mach': 7629,\n",
              " 'ländliche': 7625,\n",
              " 'lutscht': 7624,\n",
              " 'lungert': 7623,\n",
              " 'lungern': 7622,\n",
              " 'linkes': 7619,\n",
              " 'lilane': 7618,\n",
              " 'liefern': 7617,\n",
              " 'liebe': 7615,\n",
              " 'leuchtendem': 7613,\n",
              " 'lesenden': 7612,\n",
              " 'leht': 7611,\n",
              " 'lebhafte': 7607,\n",
              " 'lebendigen': 7606,\n",
              " 'laut': 7605,\n",
              " 'langärmeligem': 7600,\n",
              " 'langgeht': 7598,\n",
              " 'lang': 7597,\n",
              " 'landwirtschaftlichen': 7596,\n",
              " 'lachenden': 7594,\n",
              " 'künstlerischen': 7591,\n",
              " 'künstlerische': 7590,\n",
              " 'kurzes': 7587,\n",
              " 'kunstvoll': 7586,\n",
              " 'kulturellen': 7585,\n",
              " 'kostümierter': 7580,\n",
              " 'Ölfass': 7991,\n",
              " 'kostenlose': 7579,\n",
              " 'kosten': 7578,\n",
              " 'komisches': 7576,\n",
              " 'komisch': 7574,\n",
              " 'kniender': 7570,\n",
              " 'kippt': 7564,\n",
              " 'kegelförmigen': 7562,\n",
              " 'kastanienbraunes': 7560,\n",
              " 'jüdischer': 7554,\n",
              " 'jüdischen': 7553,\n",
              " 'jede': 7551,\n",
              " 'japanisches': 7550,\n",
              " 'japanische': 7549,\n",
              " 'irgendwohin': 7548,\n",
              " 'irgendwelche': 7547,\n",
              " 'irgendeines': 7546,\n",
              " 'irgendein': 7544,\n",
              " 'informell': 7540,\n",
              " 'immer': 7538,\n",
              " 'identisch': 7537,\n",
              " 'hütet': 7536,\n",
              " 'hängende': 7535,\n",
              " 'hochsteigt': 7534,\n",
              " 'hochkommt': 7533,\n",
              " 'hochgeschobener': 7532,\n",
              " 'hispanisch': 7529,\n",
              " 'hinunterrutscht': 7528,\n",
              " 'hinterlässt': 7527,\n",
              " 'hingefallen': 7526,\n",
              " 'hindern': 7525,\n",
              " 'herzförmigen': 7522,\n",
              " 'heruntergelassen': 7520,\n",
              " 'heruntergefallenen': 7519,\n",
              " 'herumlaufen': 7516,\n",
              " 'herrscht': 7515,\n",
              " 'herrlich': 7514,\n",
              " 'hergeht': 7512,\n",
              " 'hereinbrechenden': 7511,\n",
              " 'herausholt': 7509,\n",
              " 'herausgestreckter': 7508,\n",
              " 'herabgelassen': 7504,\n",
              " 'hellrosafarbenen': 7501,\n",
              " 'hellorangefarbenen': 7500,\n",
              " 'hellgrüner': 7497,\n",
              " 'helles': 7495,\n",
              " 'heiße': 7493,\n",
              " 'heimischen': 7492,\n",
              " 'hawaiianischer': 7491,\n",
              " 'halt': 7488,\n",
              " 'halbnackte': 7486,\n",
              " 'grün-weiße': 7481,\n",
              " 'großartigen': 7478,\n",
              " 'grob': 7476,\n",
              " 'speit': 7799,\n",
              " 'grinsender': 7475,\n",
              " 'grasbedeckten': 7473,\n",
              " 'goldgelbe': 7472,\n",
              " 'gold': 7468,\n",
              " 'glatzköpfige': 7466,\n",
              " 'getönten': 7461,\n",
              " 'getrocknet': 7460,\n",
              " 'getreten': 7459,\n",
              " 'gesprüht': 7458,\n",
              " 'unterschreibt': 7875,\n",
              " 'gesenkten': 7457,\n",
              " 'geschnittenen': 7454,\n",
              " 'geschlungen': 7453,\n",
              " 'geschleudert': 7452,\n",
              " 'gerät': 7449,\n",
              " 'geringer': 7448,\n",
              " 'gereicht': 7446,\n",
              " 'gerahmte': 7445,\n",
              " 'leeres': 7609,\n",
              " 'gepolsterte': 7443,\n",
              " 'geneigtem': 7440,\n",
              " 'gemähte': 7439,\n",
              " 'gemischter': 7437,\n",
              " 'gemischte': 7436,\n",
              " 'gemessen': 7435,\n",
              " 'gemauerten': 7434,\n",
              " 'gelbbraunem': 7433,\n",
              " 'gelassen': 7432,\n",
              " 'gekühlten': 7430,\n",
              " 'geklettert': 7427,\n",
              " 'gehört': 7426,\n",
              " 'geheimnisvoller': 7425,\n",
              " 'gegangen': 7422,\n",
              " 'gefleckten': 7419,\n",
              " 'gefleckte': 7418,\n",
              " 'gefertigt': 7417,\n",
              " 'gefalteten': 7416,\n",
              " 'gefaltet': 7415,\n",
              " 'knietiefes': 7572,\n",
              " 'geeigneten': 7414,\n",
              " 'gedeckt': 7412,\n",
              " 'gebogen': 7410,\n",
              " 'geblieben': 7409,\n",
              " 'gebeugter': 7407,\n",
              " 'gart': 7404,\n",
              " 'fügt': 7403,\n",
              " 'förmlich': 7402,\n",
              " 'frühen': 7401,\n",
              " 'frisst': 7400,\n",
              " 'freundlich': 7399,\n",
              " 'frequentierten': 7398,\n",
              " 'freies': 7396,\n",
              " 'freie': 7395,\n",
              " 'formiert': 7393,\n",
              " 'formelle': 7392,\n",
              " 'flüstert': 7391,\n",
              " 'flüstern': 7390,\n",
              " 'fließenden': 7388,\n",
              " 'handgemachten': 7489,\n",
              " 'fliederfarbener': 7386,\n",
              " 'fliederfarbenen': 7385,\n",
              " 'flickt': 7384,\n",
              " 'spinnt': 7802,\n",
              " 'flattert': 7381,\n",
              " 'flattern': 7380,\n",
              " 'flankiert': 7379,\n",
              " 'filmen': 7377,\n",
              " 'festlich': 7374,\n",
              " 'überladenen': 8004,\n",
              " 'festgemacht': 7373,\n",
              " 'festgehalten': 7372,\n",
              " 'fertiggestellten': 7371,\n",
              " 'ferngesteuerten': 7370,\n",
              " 'feil': 7369,\n",
              " 'fehlt': 7367,\n",
              " 'faltet': 7364,\n",
              " 'falsche': 7363,\n",
              " 'europäischer': 7361,\n",
              " 'europäischen': 7360,\n",
              " 'ethnischer': 7359,\n",
              " 'erörtert': 7357,\n",
              " 'erzeugt': 7356,\n",
              " 'erwachsenes': 7355,\n",
              " 'erschöpft': 7353,\n",
              " 'erledigt': 7348,\n",
              " 'erfrischenden': 7340,\n",
              " 'erforschen': 7339,\n",
              " 'entsorgt': 7336,\n",
              " 'entsetzt': 7335,\n",
              " 'entschlossen': 7334,\n",
              " 'entleert': 7332,\n",
              " 'entlangfahren': 7331,\n",
              " 'englische': 7329,\n",
              " 'em': 7328,\n",
              " 'elektronisches': 7327,\n",
              " 'elektrische': 7326,\n",
              " 'eleganten': 7325,\n",
              " 'ekstatisch': 7324,\n",
              " 'einhändigen': 7319,\n",
              " 'irgendeinen': 7545,\n",
              " 'eingepackt': 7317,\n",
              " 'eifrig': 7310,\n",
              " 'ebenso': 7309,\n",
              " 'e': 7308,\n",
              " 'dünnes': 7307,\n",
              " 'döst': 7306,\n",
              " 'durchtrennt': 7304,\n",
              " 'durchsuchen': 7303,\n",
              " 'durchsichtigen': 7302,\n",
              " 'durchsichtige': 7301,\n",
              " 'durchnässt': 7298,\n",
              " 'dreckige': 7290,\n",
              " 'ländlicher': 7626,\n",
              " 'dominiert': 7287,\n",
              " 'diversen': 7286,\n",
              " 'dient': 7284,\n",
              " 'dickem': 7283,\n",
              " 'denselben': 7281,\n",
              " 'dargestellt': 7276,\n",
              " 'dar': 7275,\n",
              " 'dann': 7274,\n",
              " 'danebenstehen': 7273,\n",
              " 'coolen': 7272,\n",
              " 'organisieren': 7684,\n",
              " 'cool': 7271,\n",
              " 'chirurgischen': 7270,\n",
              " 'bäuchlings': 7269,\n",
              " 'bäckt': 7268,\n",
              " 'burgunderfarbener': 7267,\n",
              " 'bummelt': 7266,\n",
              " 'buddeln': 7265,\n",
              " 'buckelt': 7264,\n",
              " 'brusttief': 7261,\n",
              " 'brennen': 7258,\n",
              " 'breite': 7257,\n",
              " 'braunschwarzer': 7256,\n",
              " 'braun-schwarz-weißer': 7254,\n",
              " 'boxt': 7252,\n",
              " 'bockendes': 7250,\n",
              " 'blutender': 7248,\n",
              " 'blau-weißem': 7245,\n",
              " 'blau-weiße': 7244,\n",
              " 'blau-gelber': 7243,\n",
              " 'bisschen': 7242,\n",
              " 'bin': 7241,\n",
              " 'regenbogenfarbenen': 7718,\n",
              " 'bewölkter': 7240,\n",
              " 'beworfen': 7239,\n",
              " 'riecht': 7727,\n",
              " 'bewegungslos': 7237,\n",
              " 'bewaffneten': 7236,\n",
              " 'bewachsenen': 7235,\n",
              " 'bevölkerten': 7234,\n",
              " 'betrieben': 7232,\n",
              " 'betreiben': 7231,\n",
              " 'betonierten': 7230,\n",
              " 'beteiligen': 7229,\n",
              " 'bestimmten': 7228,\n",
              " 'besten': 7226,\n",
              " 'bestaunen': 7225,\n",
              " 'besseren': 7224,\n",
              " 'besprüht': 7223,\n",
              " 'besonders': 7221,\n",
              " 'besonderen': 7220,\n",
              " 'besichtigen': 7219,\n",
              " 'demselben': 7280,\n",
              " 'besetztes': 7218,\n",
              " 'beseitigen': 7217,\n",
              " 'beschädigten': 7216,\n",
              " 'beschreibt': 7215,\n",
              " 'beschneidet': 7214,\n",
              " 'beschmierte': 7213,\n",
              " 'beschließen': 7212,\n",
              " 'berühmten': 7211,\n",
              " 'berät': 7210,\n",
              " 'bernsteinfarbener': 7209,\n",
              " 'berittener': 7208,\n",
              " 'bergauf': 7207,\n",
              " 'bemaltem': 7205,\n",
              " 'beleuchtetes': 7203,\n",
              " 'belegten': 7201,\n",
              " 'beladenes': 7200,\n",
              " 'bekleideten': 7198,\n",
              " 'beinhaltet': 7197,\n",
              " 'behandelt': 7190,\n",
              " 'beginnender': 7189,\n",
              " 'begeistertes': 7188,\n",
              " 'befördern': 7187,\n",
              " 'befreien': 7186,\n",
              " 'beeindruckenden': 7184,\n",
              " 'beeindrucken': 7183,\n",
              " 'bedruckter': 7182,\n",
              " 'bearbeiten': 7180,\n",
              " 'baumbestandenen': 7175,\n",
              " 'auszuprobieren': 7173,\n",
              " 'aussteigen': 7172,\n",
              " 'auspackt': 7170,\n",
              " 'ausgetragen': 7169,\n",
              " 'ausgestreckter': 7168,\n",
              " 'aufzuschlagen': 7165,\n",
              " 'aufzuführen': 7164,\n",
              " 'aufzufallen': 7163,\n",
              " 'aufträgt': 7160,\n",
              " 'aufgespannt': 7157,\n",
              " 'aufgesetztem': 7155,\n",
              " 'aufbäumenden': 7152,\n",
              " 'arrangiert': 7149,\n",
              " 'arbeitender': 7145,\n",
              " 'anzuzünden': 7142,\n",
              " 'anstatt': 7139,\n",
              " 'anschneiden': 7138,\n",
              " 'anhören': 7137,\n",
              " 'angestrengt': 7135,\n",
              " 'angestarrt': 7134,\n",
              " 'angeseilt': 7133,\n",
              " 'angeleuchtet': 7132,\n",
              " 'Überdachung': 7992,\n",
              " 'angelegt': 7131,\n",
              " 'angehört': 7130,\n",
              " 'ringsum': 7728,\n",
              " 'angefahren': 7128,\n",
              " 'amp': 7127,\n",
              " 'amische': 7126,\n",
              " 'alles': 7125,\n",
              " 'aktiv': 7123,\n",
              " 'aggressiv': 7122,\n",
              " 'achten': 7120,\n",
              " 'abprallen': 7116,\n",
              " 'abgelenkt': 7113,\n",
              " 'abgeleckt': 7112,\n",
              " 'abgelassen': 7111,\n",
              " 'abgefeuert': 7110,\n",
              " 'abfallenden': 7109,\n",
              " 'Zügel': 7107,\n",
              " 'Zweirad': 7104,\n",
              " 'Zustand': 7102,\n",
              " 'Zusammensein': 7100,\n",
              " 'Zuneigung': 7099,\n",
              " 'Zuhörerschaft': 7097,\n",
              " 'Zuhörern': 7096,\n",
              " 'Zugfahrt': 7093,\n",
              " 'Zirkus': 7089,\n",
              " 'Zielort': 7088,\n",
              " 'Zielfernrohr': 7087,\n",
              " 'Zickzack': 7085,\n",
              " 'Zentimeter': 7084,\n",
              " 'selbstgebastelten': 7782,\n",
              " 'Zementlaster': 7083,\n",
              " 'Zeltplatz': 7082,\n",
              " 'Zwillinge': 7105,\n",
              " 'Zaubertrick': 7079,\n",
              " 'Zahnstocher': 7076,\n",
              " 'Zahlen': 7074,\n",
              " 'Yogaübungen': 7073,\n",
              " 'Yankees': 7072,\n",
              " 'Yamaha': 7071,\n",
              " 'Würste': 7070,\n",
              " 'Würfel': 7069,\n",
              " 'Wälder': 7068,\n",
              " 'Wrestler': 7066,\n",
              " 'Wohnhauses': 7061,\n",
              " 'Winterbekleidung': 7060,\n",
              " 'hellhäutiges': 7499,\n",
              " 'flussabwärts': 7389,\n",
              " 'Wildwasserrafting': 7059,\n",
              " 'Wii': 7058,\n",
              " 'Whirlpool': 7057,\n",
              " 'Wettschwimmen': 7056,\n",
              " 'Westernstil': 7055,\n",
              " 'Werkzeuggürteln': 7054,\n",
              " 'Werkbank': 7053,\n",
              " 'Wells': 7050,\n",
              " 'Weise': 7048,\n",
              " 'Weinflasche': 7047,\n",
              " 'Weihnachtsmotiv': 7046,\n",
              " 'Weihnachtsmannmützen': 7045,\n",
              " 'Weges': 7042,\n",
              " 'Wegen': 7041,\n",
              " 'Wasserspeier': 7037,\n",
              " 'Wasserskulptur': 7036,\n",
              " 'Wasserschlacht': 7035,\n",
              " 'Wasserrohre': 7034,\n",
              " 'Zugführer': 7094,\n",
              " 'Wasserrand': 7033,\n",
              " 'Wasserlache': 7031,\n",
              " 'Wasserfontänen': 7030,\n",
              " 'Wasserbrunnen': 7029,\n",
              " 'Waschmaschinen': 7028,\n",
              " 'Warnkegel': 7024,\n",
              " 'dehnen': 7279,\n",
              " 'Wandobjekt': 7023,\n",
              " 'Wandbemalung': 7021,\n",
              " 'Walkman': 7020,\n",
              " 'Waldlandschaft': 7019,\n",
              " 'verhaftet': 7884,\n",
              " 'Waldgegend': 7018,\n",
              " 'Waggon': 7014,\n",
              " 'Vorteil': 7012,\n",
              " 'Vorräte': 7010,\n",
              " 'Vorgarten': 7008,\n",
              " 'Vorderpfoten': 7006,\n",
              " 'Vitrine': 7003,\n",
              " 'Videoaufnahme': 6999,\n",
              " 'Verwunderung': 6998,\n",
              " 'Verteidigern': 6997,\n",
              " 'Verkäuferin': 6995,\n",
              " 'Verkehrsschild': 6993,\n",
              " 'Verkehrspolizist': 6992,\n",
              " 'Verkaufstand': 6991,\n",
              " 'Verkaufsautomaten': 6990,\n",
              " 'Vergrößerungsglas': 6989,\n",
              " 'Vegas': 6988,\n",
              " 'Vase': 6987,\n",
              " 'Vader': 6986,\n",
              " 'Urinal': 6984,\n",
              " 'Ureinwohnerin': 6983,\n",
              " 'Unterhaltungskünstler': 6979,\n",
              " 'Unterführung': 6977,\n",
              " 'Unrat': 6975,\n",
              " 'Union': 6974,\n",
              " 'Unfallstelle': 6973,\n",
              " 'Umzugs': 6970,\n",
              " 'Umschlag': 6968,\n",
              " 'träg': 7853,\n",
              " 'U-Bahn-Zug': 6963,\n",
              " 'Tümpel': 6961,\n",
              " 'Tüchern': 6960,\n",
              " 'Töchter': 6958,\n",
              " 'Turnkleidung': 6954,\n",
              " 'Tuba': 6950,\n",
              " 'komischer': 7575,\n",
              " 'Trägershirt': 6949,\n",
              " 'Trägerhemden': 6948,\n",
              " 'Truthähne': 6947,\n",
              " 'Tropfen': 6945,\n",
              " 'Trittsteinen': 6944,\n",
              " 'Trinkgeld': 6943,\n",
              " 'Treffen': 6940,\n",
              " 'Trainingsanzügen': 6935,\n",
              " 'Werferhügel': 7052,\n",
              " 'Tragen': 6934,\n",
              " 'Toyota': 6933,\n",
              " 'Touristin': 6932,\n",
              " 'Torbogens': 6929,\n",
              " 'Topfpflanzen': 6928,\n",
              " 'Tomate': 6927,\n",
              " 'übersäte': 8008,\n",
              " 'Toast': 6925,\n",
              " 'Tischler': 6924,\n",
              " 'Tintenfisch': 6923,\n",
              " 'Tiers': 6921,\n",
              " 'Tiefschnee': 6920,\n",
              " 'lieben': 7616,\n",
              " 'Theaterstück': 6916,\n",
              " 'Text': 6915,\n",
              " 'unbekannte': 7868,\n",
              " 'Telefonzellen': 6913,\n",
              " 'Telefontechniker': 6912,\n",
              " 'Telefonleitung': 6911,\n",
              " 'Teenagerinnen': 6908,\n",
              " 'Teenager-Jungen': 6907,\n",
              " 'Technik': 6904,\n",
              " 'Teamkollege': 6903,\n",
              " 'Taubenschwarm': 6902,\n",
              " 'Tatort': 6901,\n",
              " 'Tasten': 6900,\n",
              " 'Taste': 6899,\n",
              " 'Tarnjacke': 6896,\n",
              " 'Tanzvorführung': 6892,\n",
              " 'Tanzstudio': 6891,\n",
              " 'Tanzende': 6890,\n",
              " 'Tank-Top': 6889,\n",
              " 'Talkshow': 6887,\n",
              " 'vierrädrigen': 7911,\n",
              " 'Takt': 6886,\n",
              " 'Taille': 6885,\n",
              " 'Tackle': 6883,\n",
              " 'Süßigkeitenladen': 6881,\n",
              " 'Söhnen': 6879,\n",
              " 'Säuglings': 6878,\n",
              " 'Säcke': 6877,\n",
              " 'Szenerie': 6876,\n",
              " 'Sweater': 6875,\n",
              " 'Suppe': 6873,\n",
              " 'Sumpf': 6872,\n",
              " 'hellrotes': 7503,\n",
              " 'Stängel': 6867,\n",
              " 'Stände': 6866,\n",
              " 'Stäbe': 6865,\n",
              " 'Studentenwohnheim': 6860,\n",
              " 'Strumpfhosen': 6858,\n",
              " 'Strommasten': 6857,\n",
              " 'Stromleitungen': 6856,\n",
              " 'Streitkräfte': 6854,\n",
              " 'Streikschildern': 6853,\n",
              " 'Streicher': 6851,\n",
              " 'Straßenmarathon': 6848,\n",
              " 'Straßenkünstlern': 6845,\n",
              " 'Straßenhändlers': 6843,\n",
              " 'Straßengeschäft': 6842,\n",
              " 'Strandspaziergang': 6840,\n",
              " 'Strandbuggy': 6838,\n",
              " 'Story': 6837,\n",
              " 'Stoppschild': 6836,\n",
              " 'Stolz': 6834,\n",
              " 'Stofftieren': 6833,\n",
              " 'Stocks': 6831,\n",
              " 'Stirnlampe': 6830,\n",
              " 'Stiften': 6829,\n",
              " 'Steuer': 6828,\n",
              " 'Sternen': 6826,\n",
              " 'Steinhaufen': 6819,\n",
              " 'Steinbrücke': 6818,\n",
              " 'Steiger': 6816,\n",
              " 'Statur': 6815,\n",
              " 'Standbild': 6812,\n",
              " 'Stammes': 6811,\n",
              " 'Stahl': 6809,\n",
              " 'Stadtbus': 6804,\n",
              " 'Stadtbrunnen': 6803,\n",
              " 'Stadions': 6801,\n",
              " 'Sprüngen': 6799,\n",
              " 'Sprinkleranlage': 6796,\n",
              " 'Sportteam': 6795,\n",
              " 'Sportstudio': 6794,\n",
              " 'Sportjacke': 6793,\n",
              " 'Sportgerät': 6791,\n",
              " 'dir': 7285,\n",
              " 'Sportfans': 6789,\n",
              " 'Spinnkurs': 6786,\n",
              " 'Spießen': 6785,\n",
              " 'Spielzeugs': 6783,\n",
              " 'Spielzeuglaster': 6780,\n",
              " 'Spielzeuggitarre': 6777,\n",
              " 'Spielzeugeisenbahn': 6775,\n",
              " 'Spielzeug-Basketballkorb': 6773,\n",
              " 'Spielkarten': 6769,\n",
              " 'Spielhaus': 6768,\n",
              " 'Spielgeräten': 6767,\n",
              " 'Spiele': 6766,\n",
              " 'Spiegelungen': 6765,\n",
              " 'Spiderman': 6764,\n",
              " 'Speiseraum': 6763,\n",
              " 'Speerwurf': 6762,\n",
              " 'Spare': 6759,\n",
              " 'Spachtel': 6758,\n",
              " 'Sonnenuntergangs': 6754,\n",
              " 'Sonnenhüten': 6752,\n",
              " 'Sonnenbad': 6751,\n",
              " 'Sonnenaufgang': 6750,\n",
              " 'Sommerkleidung': 6746,\n",
              " 'Sommerkleider': 6745,\n",
              " 'Soldatinnen': 6744,\n",
              " 'Softdrink': 6743,\n",
              " 'Soda': 6742,\n",
              " 'Skimboarden': 6737,\n",
              " 'Skibrille': 6735,\n",
              " 'Sketch': 6733,\n",
              " 'Skaterpark': 6732,\n",
              " 'Skaterbahn': 6731,\n",
              " 'Skateboard-Kunststück': 6726,\n",
              " 'Sitzender': 6724,\n",
              " 'Sitze': 6723,\n",
              " 'Sitzbank': 6722,\n",
              " 'Situation': 6721,\n",
              " 'Shampoo': 6716,\n",
              " 'Set': 6715,\n",
              " 'Sessellift': 6714,\n",
              " 'Serviette': 6713,\n",
              " 'Seifenwasser': 6705,\n",
              " 'Segeln': 6703,\n",
              " 'Segel': 6702,\n",
              " 'Seahawks': 6701,\n",
              " 'Schülerinnen': 6700,\n",
              " 'laufenden': 7603,\n",
              " 'Schäferhunde': 6697,\n",
              " 'Schwäne': 6696,\n",
              " 'Schwimmring': 6694,\n",
              " 'Schwimmflügel': 6690,\n",
              " 'Schwerarbeiter': 6688,\n",
              " 'Schwelle': 6687,\n",
              " 'Schweißen': 6685,\n",
              " 'zapft': 7960,\n",
              " 'trotzt': 7852,\n",
              " 'Schweinefleisch': 6684,\n",
              " 'Schwarzes': 6683,\n",
              " 'Schwarzen': 6682,\n",
              " 'Schutzhelme': 6681,\n",
              " 'Schuppen': 6679,\n",
              " 'Schulklasse': 6677,\n",
              " 'Thomas': 6918,\n",
              " 'Schulkindern': 6676,\n",
              " 'Schulkantine': 6675,\n",
              " 'Schulen': 6673,\n",
              " 'Schulalter': 6672,\n",
              " 'Sprühdosen': 6798,\n",
              " 'Schubkarren': 6670,\n",
              " 'Schreiben': 6667,\n",
              " 'Schraubenzieher': 6666,\n",
              " 'Tarnanzügen': 6893,\n",
              " 'Schnurrbärten': 6658,\n",
              " 'Schnellimbiss': 6657,\n",
              " 'Sturm': 6863,\n",
              " 'Schnellboot': 6656,\n",
              " 'Schneidersitz': 6655,\n",
              " 'regennassen': 7720,\n",
              " 'Schönheit': 6699,\n",
              " 'Schneiderin': 6654,\n",
              " 'Schneidbrett': 6653,\n",
              " 'Schneeschaufel': 6650,\n",
              " 'Schnees': 6649,\n",
              " 'Schneelandschaft': 6648,\n",
              " 'Schneejacke': 6647,\n",
              " 'Schneebrille': 6646,\n",
              " 'Schneeausrüstung': 6644,\n",
              " 'Schneeanzügen': 6643,\n",
              " 'Schmetterlinge': 6641,\n",
              " 'Schlittschuhe': 6640,\n",
              " 'Schleuder': 6637,\n",
              " 'Schleppe': 6636,\n",
              " 'Schleifmaschine': 6635,\n",
              " 'Schlangen': 6633,\n",
              " 'Schlagmanns': 6632,\n",
              " 'Schlagen': 6631,\n",
              " 'Schlafsäcken': 6630,\n",
              " 'Schlafanzügen': 6629,\n",
              " 'Schichten': 6622,\n",
              " 'Scheune': 6621,\n",
              " 'Scheitelpunkt': 6619,\n",
              " 'Schaulustiger': 6617,\n",
              " 'Schaulustigen': 6616,\n",
              " 'Schaukelpferd': 6615,\n",
              " 'Schaukelgestell': 6614,\n",
              " 'Schar': 6611,\n",
              " 'Schachbrett': 6610,\n",
              " 'Saxophonisten': 6609,\n",
              " 'Sanitäter': 6606,\n",
              " 'Sandsturm': 6605,\n",
              " 'Sandsack': 6604,\n",
              " 'Sandkasten': 6603,\n",
              " 'Röcke': 6598,\n",
              " 'Steinsäulen': 6823,\n",
              " 'Räder': 6597,\n",
              " 'blau-weißes': 7246,\n",
              " 'Rugby-Spiel': 6594,\n",
              " 'Rudermannschaft': 6593,\n",
              " 'Rothaarige': 6592,\n",
              " 'verglasten': 7882,\n",
              " 'Rollschuhbahn': 6589,\n",
              " 'Rodgers': 6586,\n",
              " 'errichteten': 7352,\n",
              " 'Rodeoarena': 6585,\n",
              " 'Rodeo-Clown': 6584,\n",
              " 'Ritualtanz': 6580,\n",
              " 'Ringe': 6577,\n",
              " 'Rinder': 6576,\n",
              " 'Ribbon': 6574,\n",
              " 'Rettungskraft': 6573,\n",
              " 'unbeholfen': 7867,\n",
              " 'Rettungshelfer': 6572,\n",
              " 'kahlem': 7555,\n",
              " 'Rentier': 6567,\n",
              " 'Rennmotorrad': 6566,\n",
              " 'Rennhund': 6565,\n",
              " 'Renaissance-Fest': 6563,\n",
              " 'Reishut': 6559,\n",
              " 'tanzende': 7835,\n",
              " 'Reisfeld': 6558,\n",
              " 'Regierung': 6555,\n",
              " 'Regenkleidung': 6554,\n",
              " 'Regenjacke': 6553,\n",
              " 'Rauchwolke': 6549,\n",
              " 'Rasseln': 6547,\n",
              " 'Rasiermesser': 6546,\n",
              " 'Rasierer': 6545,\n",
              " 'Rasiercreme': 6544,\n",
              " 'Querflöte': 6541,\n",
              " 'Päckchen': 6539,\n",
              " 'Punkte': 6538,\n",
              " 'Pulver': 6537,\n",
              " 'verkehrsreiche': 7887,\n",
              " 'Pullovers': 6536,\n",
              " 'Pulk': 6535,\n",
              " 'Projektorleinwand': 6532,\n",
              " 'Problem': 6531,\n",
              " 'Probe': 6530,\n",
              " 'Presslufthammer': 6528,\n",
              " 'Preisrichter': 6526,\n",
              " 'Preise': 6525,\n",
              " 'Power': 6523,\n",
              " 'Postmitarbeiter': 6522,\n",
              " 'gemustertem': 7438,\n",
              " 'Ponchos': 6518,\n",
              " 'Polo-Hemd': 6517,\n",
              " 'Polizeifahrzeug': 6515,\n",
              " 'Polizeibeamter': 6514,\n",
              " 'Polizeiauto': 6513,\n",
              " 'Plüschtieren': 6512,\n",
              " 'Plattenladen': 6511,\n",
              " 'Plastikwanne': 6510,\n",
              " 'Plastikflasche': 6508,\n",
              " 'Plastikeimer': 6507,\n",
              " 'Plastikbecher': 6506,\n",
              " 'Pinien': 6503,\n",
              " 'Pinguine': 6502,\n",
              " 'Pilz': 6501,\n",
              " 'Piercings': 6500,\n",
              " 'Piano': 6498,\n",
              " 'Pfote': 6496,\n",
              " 'Pferdeschwänzen': 6494,\n",
              " 'Pferch': 6492,\n",
              " 'Pfeiler': 6491,\n",
              " 'Pfauenkostüm': 6490,\n",
              " 'Pfau': 6489,\n",
              " 'Pfannkuchen': 6487,\n",
              " 'Pfadfindern': 6486,\n",
              " 'Personenzug': 6484,\n",
              " 'Perle': 6483,\n",
              " 'Pedale': 6480,\n",
              " 'Pedal': 6479,\n",
              " 'Pavillon': 6478,\n",
              " 'Patient': 6477,\n",
              " 'Passagier': 6476,\n",
              " 'vesammelt': 7908,\n",
              " 'Partnerin': 6475,\n",
              " 'Partner': 6474,\n",
              " 'Parkuhr': 6472,\n",
              " 'Parkgarage': 6471,\n",
              " 'Parken': 6470,\n",
              " 'Paris': 6468,\n",
              " 'Pappteller': 6467,\n",
              " 'Pappschachtel': 6466,\n",
              " 'Papierhut': 6464,\n",
              " 'Papagei': 6463,\n",
              " 'Pain': 6459,\n",
              " 'Outdoor-Bekleidung': 6457,\n",
              " 'Orten': 6456,\n",
              " 'Orioles': 6455,\n",
              " 'Orgel': 6454,\n",
              " 'Ordner': 6453,\n",
              " 'Orchesters': 6452,\n",
              " 'Opfer': 6451,\n",
              " 'Open-Air-Konzert': 6450,\n",
              " 'Open-Air-Festival': 6449,\n",
              " 'Olympische': 6448,\n",
              " 'Ohrschützern': 6446,\n",
              " 'Ohrringen': 6445,\n",
              " 'Objektiv': 6442,\n",
              " 'Oberleitungsfahrzeug': 6441,\n",
              " 'Obama': 6440,\n",
              " 'Nägel': 6437,\n",
              " 'Notre': 6436,\n",
              " 'Notenblättern': 6434,\n",
              " 'Notenblatt': 6433,\n",
              " 'Nonnentracht': 6430,\n",
              " 'Nische': 6428,\n",
              " 'Bekleidungsgeschäft': 3195,\n",
              " 'Beim': 3194,\n",
              " 'unterhalten': 189,\n",
              " 'Bauernhof': 3192,\n",
              " 'Wachmann': 2411,\n",
              " 'BMX-Rad': 3180,\n",
              " 'Außenseite': 3179,\n",
              " 'Softballspieler': 4862,\n",
              " 'Auszeichnung': 3178,\n",
              " 'Capris': 5746,\n",
              " 'Ausflug': 3177,\n",
              " 'musiziert': 7656,\n",
              " 'Aquarium': 2790,\n",
              " 'Krügen': 6231,\n",
              " 'Karton': 1976,\n",
              " 'Massen': 3846,\n",
              " 'Ansammlung': 3171,\n",
              " 'verläuft': 5432,\n",
              " 'Angel': 3170,\n",
              " 'free': 7394,\n",
              " 'paar': 306,\n",
              " 'Bulldozer': 3205,\n",
              " 'Afrikanische': 3166,\n",
              " 'Drachen': 1340,\n",
              " 'wählt': 3153,\n",
              " 'Einige': 432,\n",
              " 'wir': 3151,\n",
              " 'wenig': 3149,\n",
              " 'Dosen': 2543,\n",
              " 'Bier': 467,\n",
              " 'Wandgemälde': 7022,\n",
              " 'ärmellosem': 3157,\n",
              " 'Tourbus': 6931,\n",
              " 'leisten': 5296,\n",
              " 'tätowierter': 3134,\n",
              " 'schick': 7749,\n",
              " ' ': 674,\n",
              " 'Schaufensterbummel': 6613,\n",
              " 'trocknet': 3133,\n",
              " 'teilnimmt': 3131,\n",
              " ...}"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SRC.vocab.get_stoi()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4PiQ3HR9WKP"
      },
      "source": [
        "* 한 문장에 포함된 단어가 연속적으로 **LSTM**에 입력되어야 합니다.\n",
        "    * 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋습니다.\n",
        "    * 이를 위해 BucketIterator를 사용합니다.\n",
        "    * **배치 크기(batch size)**: 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "KM6EVV2t9BHd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator,test_iterator,valid_iterator = iter(Multi30k(root='.data', split=('train','test','valid'), language_pair=(\"de\", \"en\")))\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_iterator, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_iterator, batch_size=BATCH_SIZE, shuffle=False)\n",
        "valid_dataloader = DataLoader(valid_iterator, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gINFAYgxxgPI",
        "outputId": "b80a56be-6ce5-4f6f-ee81-4666eaf0c1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "128\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(train_dataloader):\n",
        "    print(i)\n",
        "    print(len(batch[0]))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMJmn_fZmBWG",
        "outputId": "94598972-f8db-4d48-d5e6-c5654d2ecd6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "('Musiker üben und unter Anleitung eines Dirigenten.', 'Musicians are practicing as directed by a director.')\n",
            "1\n",
            "('Zwei kleine Jungen fahren in Spielzeug-Jeep an Erwachsenen vorbei.', 'Two little boys are driving past adults in their toy jeep.')\n",
            "2\n",
            "('Ein junger Mann mit seinem Dirtbike befindet sich in einem Bach.', 'A young man is in a stream with his dirt bike.')\n",
            "3\n",
            "('Mehrere Personen, die einen Pfad entlang gehen.', 'Several people are walking down a trail.')\n",
            "4\n",
            "('Ein kleines Mädchen klettert eine Felswand hoch.', 'A small girl climbs a rock wall.')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:249: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
          ]
        }
      ],
      "source": [
        "cnt=0\n",
        "a=[]\n",
        "for i, batch in enumerate(train_iterator):\n",
        "    a.append(batch)\n",
        "    print(i)\n",
        "    print(batch)\n",
        "    cnt+=1\n",
        "    if cnt>4:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cr-UGDKCmd-V",
        "outputId": "b656ffa1-6a8b-4691-e9ab-e7c278026aaa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Musiker üben und unter Anleitung eines Dirigenten.'"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VNXqHZCKm4en",
        "outputId": "551d8541-53b5-4736-ee0a-24db35ac22d0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Musicians are practicing as directed by a director.'"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j14n9yPMvryi",
        "outputId": "8fd56a86-a1ff-4c7e-c249-e43b2850fb51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3, 0, 44, 0, 129, 8, 1040, 0]"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SRC(tokenize_de(a[0][0].lower()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oguDx24e-3Db"
      },
      "source": [
        "#### **인코더(Encoder) 아키텍처**\n",
        "\n",
        "* 주어진 소스 문장을 **문맥 벡터(context vector)로 인코딩**합니다.\n",
        "* LSTM은 hidden state과 cell state을 반환합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원핫 인코딩 차원\n",
        "    * **embed_dim**: 임베딩(embedding) 차원\n",
        "    * **hidden_dim**: 히든 상태(hidden state) 차원\n",
        "    * **n_layers**: RNN 레이어의 개수\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Ac28d5DL_ceY"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 인코더(Encoder) 아키텍처 정의\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding)을 특정 차원의 임베딩으로 매핑하는 레이어\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n",
        "        \n",
        "        # 드롭아웃(dropout)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 인코더는 소스 문장을 입력으로 받아 문맥 벡터(context vector)를 반환        \n",
        "    def forward(self, src):\n",
        "        # src: [단어 개수, 배치 크기]: 각 단어의 인덱스(index) 정보\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs: [단어 개수, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "        # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "\n",
        "        # 문맥 벡터(context vector) 반환\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pHj4IlvBzPe"
      },
      "source": [
        "#### **디코더(Decoder) 아키텍처**\n",
        "\n",
        "* 주어진 문맥 벡터(context vector)를 **타겟 문장으로 디코딩**합니다.\n",
        "* LSTM은 hidden state과 cell state을 반환합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원핫 인코딩 차원\n",
        "    * **embed_dim**: 임베딩(embedding) 차원\n",
        "    * **hidden_dim**: 히든 상태(hidden state) 차원\n",
        "    * **n_layers**: RNN 레이어의 개수\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "fusf6_9yDmfM"
      },
      "outputs": [],
      "source": [
        "# 디코더(Decoder) 아키텍처 정의\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding) 말고 특정 차원의 임베딩으로 매핑하는 레이어\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n",
        "        \n",
        "        # FC 레이어 (인코더와 구조적으로 다른 부분)\n",
        "        self.output_dim = output_dim\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        # 드롭아웃(dropout)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환     \n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input: [배치 크기]: 단어의 개수는 항상 1개이도록 구현\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]\n",
        "        # cell = context: [레이어 개수, 배치 크기, 히든 차원]\n",
        "        input = input.unsqueeze(0)\n",
        "        # input: [단어 개수 = 1, 배치 크기]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # output: [단어 개수 = 1, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "        # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "\n",
        "        # 단어 개수는 어차피 1개이므로 차원 제거\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [배치 크기, 출력 차원]\n",
        "        \n",
        "        # (현재 출력 단어, 현재까지의 모든 단어의 정보, 현재까지의 모든 단어의 정보)\n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j17WxtpJI_9V"
      },
      "source": [
        "#### **Seq2Seq 아키텍처**\n",
        "\n",
        "* 앞서 정의한 인코더(encoder)와 디코더(decoder)를 가지고 있는 하나의 아키텍처입니다.\n",
        "    * **인코더(encoder)**: 주어진 소스 문장을 문맥 벡터(context vector)로 인코딩합니다.\n",
        "    * **디코더(decoder)**: 주어진 문맥 벡터(context vector)를 타겟 문장으로 디코딩합니다.\n",
        "    * 단, **디코더는 한 단어씩** 넣어서 한 번씩 결과를 구합니다.\n",
        "* **Teacher forcing**: 디코더의 예측(prediction)을 다음 입력으로 사용하지 않고, 실제 목표 출력(ground-truth)을 다음 입력으로 사용하는 기법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "vsA6C6B5Glhc"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    # 학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio를 넣기\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [단어 개수, 배치 크기]\n",
        "        # trg: [단어 개수, 배치 크기]\n",
        "        # 먼저 인코더를 거쳐 문맥 벡터(context vector)를 추출\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # 디코더(decoder)의 최종 결과를 담을 텐서 객체 만들기\n",
        "        trg_len = trg.shape[0] # 단어 개수\n",
        "        batch_size = trg.shape[1] # 배치 크기\n",
        "        trg_vocab_size = self.decoder.output_dim # 출력 차원\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # 첫 번째 입력은 항상 <sos> 토큰\n",
        "        input = trg[0, :]\n",
        "\n",
        "        # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보\n",
        "            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n",
        "\n",
        "            # teacher_forcing_ratio: 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyXRWyDrHYSB"
      },
      "source": [
        "#### **학습(Training)**\n",
        "\n",
        "* 하이퍼 파라미터 설정 및 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qVsGIVvzMZ-N"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENCODER_EMBED_DIM = 256\n",
        "DECODER_EMBED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT_RATIO = 0.5\n",
        "DEC_DROPOUT_RATIO = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0WM3urPiIE1T"
      },
      "outputs": [],
      "source": [
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n",
        "dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n",
        "\n",
        "# Seq2Seq 객체 선언\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcnWnXGIMwHk"
      },
      "source": [
        "* 논문의 내용대로 $\\mathcal{U}(-0.08, 0.08)$의 값으로 **모델 가중치 파라미터 초기화**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdfqma4uMaoI",
        "outputId": "78f3a514-7e2a-404b-99b4-f2c81bcc57f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(8012, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(6189, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=6189, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUwj0UgIS6E"
      },
      "source": [
        "* 학습 및 평가 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BeqqI7xfM71V"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "# TRG_PAD_IDX = TRG.vocab.get_stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl-BLpcGWP5z",
        "outputId": "8664cec2-ff51-4203-9d32-c008273a8d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kinder, darunter eines mit einem geschminkten Gesicht, streicheln kleine Schildkröten, die im grünen Gras krabbeln.\n",
            "[3, 0, 119, 82, 21, 18, 7, 0, 113, 3581, 7, 206, 3062, 5, 9, 44, 1064, 7, 66]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:249: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
          ]
        }
      ],
      "source": [
        "for i,batch in enumerate(train_dataloader):\n",
        "    debatch,enbatch = batch[0],batch[1]\n",
        "    for j in debatch:\n",
        "        print(j)\n",
        "        print(SRC(tokenize_de(j)))\n",
        "        break\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "darFKcKAyx9q",
        "outputId": "1f8e21ce-f329-4f68-d0b4-37c4aef68e48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:249: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-f9b2e50d61c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_de\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTRG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-f9b2e50d61c6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_de\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTRG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab/vocab.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0massociated\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Token kombiniert not found and default index is not set"
          ]
        }
      ],
      "source": [
        "##### dataloader에서 나온 batch를 Tensor로 바꿔줄 필요가...\n",
        "##### 어떻게 하지\n",
        "src=torch.zeros((1))\n",
        "for i, batch in enumerate(train_dataloader):\n",
        "        src=torch.stack([src,torch.tensor([SRC(tokenize_de(batch[0][j])) for j in range(len(batch))]).unsqueeze(1)])\n",
        "        trg = torch.tensor([TRG(tokenize_en(batch[1][j])) for j in range(len(batch))]).unsqueeze(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "SXo7ZOclNG2-"
      },
      "outputs": [],
      "source": [
        "# 모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = torch.tensor([SRC(tokenize_de(batch[0][j])) for j in range(len(batch))]).unsqueeze(1).to(device)\n",
        "        trg = torch.tensor([TRG(tokenize_en(batch[1][j])) for j in range(len(batch))]).unsqueeze(1).to(device)\n",
        "\n",
        "        print(type(src))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "        # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        # 출력 단어의 인덱스 0은 사용하지 않음\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
        "        trg = trg[1:].view(-1)\n",
        "        # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
        "        \n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "        \n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "        \n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "DzR8hm9HQ1gb"
      },
      "outputs": [],
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch[0]\n",
        "            trg = batch[1]\n",
        "            \n",
        "            # 평가할 때 teacher forcing는 사용하지 않음\n",
        "            output = model(src, trg, 0)\n",
        "            # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            # 출력 단어의 인덱스 0은 사용하지 않음\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
        "            trg = trg[1:].view(-1)\n",
        "            # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76X7pR1cLtAl"
      },
      "source": [
        "* 학습(training) 및 검증(validation) 진행\n",
        "    * **학습 횟수(epoch)**: 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "iVGWe9VtSwx0"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "EMMkAnGeSyMW",
        "outputId": "ea2e95e1-812e-4337-cdbb-d78421ccbc92"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-45742100cd8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 시작 시간 기록\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-a7f0bb9f621e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 전체 학습 데이터를 확인하며\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_de\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-d85e855302dc>\u001b[0m in \u001b[0;36mtokenize_de\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 독일어(Deutsch) 문장을 토큰화한 뒤에 순서를 뒤집는 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_de\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspacy_de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 영어(English) 문장을 토큰화 하는 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got tuple)"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'seq2seq.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvYlZ85ZRUya"
      },
      "outputs": [],
      "source": [
        "# 학습된 모델 저장\n",
        "from google.colab import files\n",
        "\n",
        "files.download('seq2seq.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6d2isoPL_P0"
      },
      "source": [
        "#### **모델 최종 테스트(testing) 결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfs5GYSpUGeU"
      },
      "outputs": [],
      "source": [
        "!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/ERgwTMYWR7FMhApROaNvZREBTjEDi00ttSzt8ZNj1PS_5g?download=1 -O seq2seq.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co3YMQ2NS0Ia"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('seq2seq.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMvcdNR7YqAo"
      },
      "source": [
        "#### **나만의 데이터로 모델 사용해보기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjHJkeznS1oS"
      },
      "outputs": [],
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    print(f\"전체 소스 토큰: {tokens}\")\n",
        "\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    print(f\"소스 문장 인덱스: {src_indexes}\")\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "        # <eos>를 만나는 순간 끝\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
        "    return trg_tokens[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OSgbkh0Vkq7"
      },
      "outputs": [],
      "source": [
        "example_idx = 10\n",
        "\n",
        "src = vars(test_dataset.examples[example_idx])['src']\n",
        "trg = vars(test_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(f'타겟 문장: {trg}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR4mSubPXOJV"
      },
      "outputs": [],
      "source": [
        "src = tokenize_de(\"Guten Abend.\")\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
