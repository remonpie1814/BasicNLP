{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc2pgM0w9Xa1"
      },
      "source": [
        "#### **Sequence to Sequence Learning with Neural Networks (NIPS 2014)** 실습\n",
        "* 본 코드는 기본적으로 **Seq2Seq** 논문의 내용을 따릅니다.\n",
        "    * 본 논문은 **딥러닝 기반의 자연어 처리** 기법의 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다.\n",
        "    * 2020년 기준 가장 뛰어난 번역 모델은 Seq2Seq가 아닌 **Transformer 기반의 모델**입니다.\n",
        "* 코드 실행 전에 **[런타임]** → **[런타임 유형 변경]** → 유형을 **GPU**로 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_UOI3Xm4U4M"
      },
      "source": [
        "#### **데이터 전처리(Preprocessing)**\n",
        "\n",
        "* **spaCy 라이브러리**: 문장의 토큰화(tokenization), 태깅(tagging) 등의 전처리 기능을 위한 라이브러리\n",
        "  * 영어(Engilsh)와 독일어(Deutsch) 전처리 모듈 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8SEN31g34aX"
      },
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7u-Xt2c4WV8"
      },
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load(\"en_core_web_sm\") # 영어 토큰화(tokenization)\n",
        "spacy_de = spacy.load(\"de_core_news_sm\") # 독일어 토큰화(tokenization)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vnjcXO84aRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0832667d-7b20-49e0-ea4d-c08761214c8a"
      },
      "source": [
        "# 간단히 토큰화(tokenization) 기능 써보기\n",
        "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "    print(f\"인덱스 {i}: {token.text}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: am\n",
            "인덱스 2: a\n",
            "인덱스 3: graduate\n",
            "인덱스 4: student\n",
            "인덱스 5: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hskqq3f4-YT"
      },
      "source": [
        "* 영어(English) 및 독일어(Deutsch) **토큰화 함수** 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EH0gEEb4iTj"
      },
      "source": [
        "# 독일어(Deutsch) 문장을 토큰화한 뒤에 순서를 뒤집는 함수\n",
        "def tokenize_de(text):\n",
        "    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "# 영어(English) 문장을 토큰화 하는 함수\n",
        "def tokenize_en(text):\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-fAQN1O6_a9"
      },
      "source": [
        "* **필드(field)** 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시합니다.\n",
        "* 번역 목표\n",
        "    * 소스(SRC): 독일어\n",
        "    * 목표(TRG): 영어"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zLh_--xWZSc",
        "outputId": "a671ad0a-e3ed-4160-8707-1da16e90133f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchtext) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext"
      ],
      "metadata": {
        "id": "MVsQFWVmazAM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3MN7tLQ8wzG"
      },
      "source": [
        "* 대표적인 영어-독어 번역 데이터셋인 **Multi30k**를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "vkAL7LBJdmzG",
        "outputId": "1698d225-3903-4396-e1c9-8111ef66b208"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 28.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 69.6 MB/s \n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchdata) (4.1.1)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 54.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Installing collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.5.1 torchdata-0.4.1 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaEW2EpY72jt"
      },
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = Multi30k(root='.data', split=('train','test','valid'), language_pair=(\"de\", \"en\"))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en = []\n",
        "de = []\n",
        "for label, line in train_dataset:\n",
        "    de+=[tokenize_de(label)]\n",
        "    en+=[tokenize_en(line)]"
      ],
      "metadata": {
        "id": "41tiyqEDeF8N"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJQmiqRXkn-4",
        "outputId": "2521fe99-78a8-4ab1-a425-d464d51b8dd6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Two',\n",
              "  'young',\n",
              "  ',',\n",
              "  'White',\n",
              "  'males',\n",
              "  'are',\n",
              "  'outside',\n",
              "  'near',\n",
              "  'many',\n",
              "  'bushes',\n",
              "  '.'],\n",
              " ['Several',\n",
              "  'men',\n",
              "  'in',\n",
              "  'hard',\n",
              "  'hats',\n",
              "  'are',\n",
              "  'operating',\n",
              "  'a',\n",
              "  'giant',\n",
              "  'pulley',\n",
              "  'system',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target=torchtext.vocab.build_vocab_from_iterator(en,min_freq=2)\n",
        "source=torchtext.vocab.build_vocab_from_iterator(de,min_freq=2)"
      ],
      "metadata": {
        "id": "QUwwB9bMeTNG"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(source.vocab))\n",
        "print(len(target.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crY-cQsygvyz",
        "outputId": "ddc334da-b18c-4090-ca26-34d3ac014197"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6187\n",
            "8010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source.vocab.get_stoi()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-B_3ZNMiPap",
        "outputId": "642b7b5f-6687-4b6d-ebec-9443a0425225"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'‘': 8009,\n",
              " 'üppigen': 8008,\n",
              " 'überwacht': 8006,\n",
              " 'überlegt': 8002,\n",
              " 'überfliegt': 7998,\n",
              " 'örtlicher': 7997,\n",
              " 'öffentliches': 7996,\n",
              " 'Überschlag': 7993,\n",
              " 'Überreste': 7992,\n",
              " 'Überhang': 7991,\n",
              " 'Äste': 7986,\n",
              " 'zwirbelt': 7983,\n",
              " 'zweispurigen': 7981,\n",
              " 'zusammenpassenden': 7979,\n",
              " 'zusammengebaut': 7977,\n",
              " 'zugedeckt': 7974,\n",
              " 'zufährt': 7973,\n",
              " 'zuerst': 7972,\n",
              " 'zotteliger': 7971,\n",
              " 'zerzaust': 7970,\n",
              " 'zerrissenen': 7969,\n",
              " 'zerlumpter': 7968,\n",
              " 'zerlegen': 7967,\n",
              " 'zerbrochenen': 7963,\n",
              " 'zerbrochene': 7962,\n",
              " 'zerbricht': 7961,\n",
              " 'zehn': 7958,\n",
              " 'you': 7956,\n",
              " 'wütend': 7955,\n",
              " 'wunderschön': 7951,\n",
              " 'wodurch': 7948,\n",
              " 'wirbeln': 7946,\n",
              " 'winterlichen': 7945,\n",
              " 'winkend': 7944,\n",
              " 'wichtiges': 7941,\n",
              " 'wetteifert': 7940,\n",
              " 'werdendem': 7939,\n",
              " 'wer': 7938,\n",
              " 'wenden': 7937,\n",
              " 'weißhaarige': 7935,\n",
              " 'weißene': 7934,\n",
              " 'weiß-rotes': 7933,\n",
              " 'weiß-braunem': 7932,\n",
              " 'weinroten': 7930,\n",
              " 'weiblich': 7929,\n",
              " 'wehrt': 7928,\n",
              " 'wegschaut': 7927,\n",
              " 'wartende': 7926,\n",
              " 'vorzubereiten': 7922,\n",
              " 'vornehmen': 7919,\n",
              " 'vordere': 7917,\n",
              " 'vorbeizukommen': 7916,\n",
              " 'vorbeiläuft': 7915,\n",
              " 'vorbeikommt': 7914,\n",
              " 'vorbeifahrenden': 7913,\n",
              " 'visiert': 7911,\n",
              " 'violette': 7910,\n",
              " 'vierte': 7909,\n",
              " 'vielfältige': 7907,\n",
              " 'vieler': 7906,\n",
              " 'verzweifelt': 7904,\n",
              " 'vertieft': 7899,\n",
              " 'versorgt': 7898,\n",
              " 'verschlungenen': 7893,\n",
              " 'verschiedenfarbiger': 7892,\n",
              " 'verregneten': 7890,\n",
              " 'verpasst': 7888,\n",
              " 'vermischt': 7887,\n",
              " 'verlegen': 7886,\n",
              " 'verheirateter': 7882,\n",
              " 'vergraben': 7880,\n",
              " 'vereinzelten': 7878,\n",
              " 'verborgen': 7877,\n",
              " 'verbeugen': 7875,\n",
              " 'unterstützen': 7873,\n",
              " 'unterirdischen': 7871,\n",
              " 'unglückliches': 7870,\n",
              " 'ungepflegten': 7868,\n",
              " 'umgekippt': 7862,\n",
              " 'umgedrehte': 7860,\n",
              " 'umgebene': 7859,\n",
              " 'umdreht': 7857,\n",
              " 'türkise': 7856,\n",
              " 'töpfert': 7855,\n",
              " 'tätowierten': 7854,\n",
              " 'trösten': 7851,\n",
              " 'trommeln': 7848,\n",
              " 'trockene': 7847,\n",
              " 'transparenten': 7844,\n",
              " 'verschwommenen': 7896,\n",
              " 'traditionellem': 7843,\n",
              " 'toten': 7840,\n",
              " 'tollt': 7839,\n",
              " 'to': 7838,\n",
              " 'tiefblauen': 7837,\n",
              " 'testet': 7835,\n",
              " 'teilnehmende': 7834,\n",
              " 'süßer': 7830,\n",
              " 'sumpfiges': 7829,\n",
              " 'sumpfigen': 7828,\n",
              " 'stürmt': 7827,\n",
              " 'stürmischem': 7826,\n",
              " 'stämmiger': 7825,\n",
              " 'struppiger': 7823,\n",
              " 'striegelt': 7822,\n",
              " 'stilles': 7817,\n",
              " 'stiert': 7816,\n",
              " 'sticht': 7815,\n",
              " 'steuern': 7814,\n",
              " 'stets': 7813,\n",
              " 'steiniges': 7812,\n",
              " 'steigenden': 7811,\n",
              " 'stattfindet': 7810,\n",
              " 'stattfindenden': 7809,\n",
              " 'starten': 7807,\n",
              " 'startbereit': 7806,\n",
              " 'starker': 7805,\n",
              " 'spät': 7802,\n",
              " 'spitzen': 7800,\n",
              " 'speist': 7795,\n",
              " 'speisen': 7794,\n",
              " 'sorgt': 7791,\n",
              " 'sonniger': 7790,\n",
              " 'sogar': 7789,\n",
              " 'singend': 7788,\n",
              " 'silbernem': 7787,\n",
              " 'silberfarbene': 7786,\n",
              " 'signiert': 7785,\n",
              " 'sichtbare': 7784,\n",
              " 'senkrechte': 7782,\n",
              " 'sendet': 7781,\n",
              " 'selbstgebauten': 7780,\n",
              " 'sehe': 7777,\n",
              " 'schürt': 7774,\n",
              " 'schön': 7773,\n",
              " 'schäbig': 7772,\n",
              " 'schwimmende': 7769,\n",
              " 'schubst': 7763,\n",
              " 'schrägen': 7762,\n",
              " 'schroffen': 7761,\n",
              " 'schreienden': 7760,\n",
              " 'schottischer': 7758,\n",
              " 'schnüffeln': 7757,\n",
              " 'schlittert': 7753,\n",
              " 'schlammiges': 7750,\n",
              " 'schlammigem': 7749,\n",
              " 'schirmen': 7747,\n",
              " 'scheucht': 7745,\n",
              " 'schert': 7744,\n",
              " 'schafft': 7743,\n",
              " 'sausen': 7742,\n",
              " 'sanft': 7740,\n",
              " 's': 7739,\n",
              " 'rötlichem': 7738,\n",
              " 'rund': 7736,\n",
              " 'rot-grünen': 7734,\n",
              " 'rot-goldenen': 7733,\n",
              " 'rot-gelben': 7731,\n",
              " 'rosa-grünen': 7729,\n",
              " 'rodelt': 7728,\n",
              " 'riss': 7726,\n",
              " 'reißen': 7720,\n",
              " 'regenbogenfarbigen': 7716,\n",
              " 'reckt': 7714,\n",
              " 'rechteckigen': 7713,\n",
              " 'reagiert': 7712,\n",
              " 'rausgeworfen': 7711,\n",
              " 'rauen': 7710,\n",
              " 'rasten': 7709,\n",
              " 'rasiertem': 7708,\n",
              " 'pusten': 7704,\n",
              " 'purpurroten': 7703,\n",
              " 'prächtige': 7701,\n",
              " 'professionell': 7699,\n",
              " 'pro': 7698,\n",
              " 'presst': 7697,\n",
              " 'praktizieren': 7695,\n",
              " 'plaudern': 7694,\n",
              " 'pinkfarbenes': 7692,\n",
              " 'schreibend': 7759,\n",
              " 'pink-weißem': 7691,\n",
              " 'patrouilliert': 7687,\n",
              " 'patriotischen': 7686,\n",
              " 'passender': 7684,\n",
              " 'passendem': 7683,\n",
              " 'passende': 7682,\n",
              " 'organgefarbenen': 7680,\n",
              " 'orange-weiß': 7678,\n",
              " 'orange-grauen': 7677,\n",
              " 'orange-blauen': 7676,\n",
              " 'operiert': 7675,\n",
              " 'nähen': 7674,\n",
              " 'nuckelt': 7671,\n",
              " 'normaler': 7670,\n",
              " 'niemand': 7669,\n",
              " 'neun': 7667,\n",
              " 'neugieriges': 7666,\n",
              " 'neugeborenen': 7665,\n",
              " 'neugeborene': 7664,\n",
              " 'neonfarbenen': 7662,\n",
              " 'nassgespritzt': 7661,\n",
              " 'nachgehen': 7659,\n",
              " 'nachdenklich': 7658,\n",
              " 'mürrisch': 7657,\n",
              " 'muslimischer': 7655,\n",
              " 'typischen': 7852,\n",
              " 'muslimische': 7654,\n",
              " 'musikalische': 7652,\n",
              " 'moosbedeckten': 7651,\n",
              " 'molliges': 7650,\n",
              " 'modischer': 7649,\n",
              " 'modernes': 7647,\n",
              " 'modelliert': 7645,\n",
              " 'mobile': 7644,\n",
              " 'mittlerem': 7643,\n",
              " 'mischen': 7641,\n",
              " 'militärischen': 7640,\n",
              " 'mexikanische': 7639,\n",
              " 'meine': 7634,\n",
              " 'mehrfarbiger': 7633,\n",
              " 'marineblauer': 7630,\n",
              " 'manövrieren': 7629,\n",
              " 'malerischen': 7628,\n",
              " 'mach': 7626,\n",
              " 'ländliche': 7622,\n",
              " 'lutscht': 7621,\n",
              " 'lungert': 7620,\n",
              " 'lungern': 7619,\n",
              " 'linkes': 7616,\n",
              " 'lilane': 7615,\n",
              " 'liefern': 7614,\n",
              " 'liebe': 7612,\n",
              " 'leuchtendem': 7610,\n",
              " 'lesenden': 7609,\n",
              " 'leht': 7608,\n",
              " 'lebhafte': 7604,\n",
              " 'lebendigen': 7603,\n",
              " 'laut': 7602,\n",
              " 'langärmeligem': 7597,\n",
              " 'langgeht': 7595,\n",
              " 'lang': 7594,\n",
              " 'landwirtschaftlichen': 7593,\n",
              " 'lachenden': 7591,\n",
              " 'künstlerischen': 7588,\n",
              " 'künstlerische': 7587,\n",
              " 'kurzes': 7584,\n",
              " 'kunstvoll': 7583,\n",
              " 'kulturellen': 7582,\n",
              " 'kostümierter': 7577,\n",
              " 'Ölfass': 7988,\n",
              " 'kostenlose': 7576,\n",
              " 'kosten': 7575,\n",
              " 'komisches': 7573,\n",
              " 'komisch': 7571,\n",
              " 'kniender': 7567,\n",
              " 'kippt': 7561,\n",
              " 'kegelförmigen': 7559,\n",
              " 'kastanienbraunes': 7557,\n",
              " 'jüdischer': 7551,\n",
              " 'jüdischen': 7550,\n",
              " 'jede': 7548,\n",
              " 'japanisches': 7547,\n",
              " 'japanische': 7546,\n",
              " 'irgendwohin': 7545,\n",
              " 'irgendwelche': 7544,\n",
              " 'irgendeines': 7543,\n",
              " 'irgendein': 7541,\n",
              " 'informell': 7537,\n",
              " 'immer': 7535,\n",
              " 'identisch': 7534,\n",
              " 'hütet': 7533,\n",
              " 'hängende': 7532,\n",
              " 'hochsteigt': 7531,\n",
              " 'hochkommt': 7530,\n",
              " 'hochgeschobener': 7529,\n",
              " 'hispanisch': 7526,\n",
              " 'hinunterrutscht': 7525,\n",
              " 'hinterlässt': 7524,\n",
              " 'hingefallen': 7523,\n",
              " 'hindern': 7522,\n",
              " 'herzförmigen': 7519,\n",
              " 'heruntergelassen': 7517,\n",
              " 'heruntergefallenen': 7516,\n",
              " 'herumlaufen': 7513,\n",
              " 'herrscht': 7512,\n",
              " 'herrlich': 7511,\n",
              " 'hergeht': 7509,\n",
              " 'hereinbrechenden': 7508,\n",
              " 'herausholt': 7506,\n",
              " 'herausgestreckter': 7505,\n",
              " 'herabgelassen': 7501,\n",
              " 'hellrosafarbenen': 7498,\n",
              " 'hellorangefarbenen': 7497,\n",
              " 'hellgrüner': 7494,\n",
              " 'helles': 7492,\n",
              " 'heiße': 7490,\n",
              " 'heimischen': 7489,\n",
              " 'hawaiianischer': 7488,\n",
              " 'halt': 7485,\n",
              " 'halbnackte': 7483,\n",
              " 'grün-weiße': 7478,\n",
              " 'großartigen': 7475,\n",
              " 'grob': 7473,\n",
              " 'speit': 7796,\n",
              " 'grinsender': 7472,\n",
              " 'grasbedeckten': 7470,\n",
              " 'goldgelbe': 7469,\n",
              " 'gold': 7465,\n",
              " 'glatzköpfige': 7463,\n",
              " 'getönten': 7458,\n",
              " 'getrocknet': 7457,\n",
              " 'getreten': 7456,\n",
              " 'gesprüht': 7455,\n",
              " 'unterschreibt': 7872,\n",
              " 'gesenkten': 7454,\n",
              " 'geschnittenen': 7451,\n",
              " 'geschlungen': 7450,\n",
              " 'geschleudert': 7449,\n",
              " 'gerät': 7446,\n",
              " 'geringer': 7445,\n",
              " 'gereicht': 7443,\n",
              " 'gerahmte': 7442,\n",
              " 'leeres': 7606,\n",
              " 'gepolsterte': 7440,\n",
              " 'geneigtem': 7437,\n",
              " 'gemähte': 7436,\n",
              " 'gemischter': 7434,\n",
              " 'gemischte': 7433,\n",
              " 'gemessen': 7432,\n",
              " 'gemauerten': 7431,\n",
              " 'gelbbraunem': 7430,\n",
              " 'gelassen': 7429,\n",
              " 'gekühlten': 7427,\n",
              " 'geklettert': 7424,\n",
              " 'gehört': 7423,\n",
              " 'geheimnisvoller': 7422,\n",
              " 'gegangen': 7419,\n",
              " 'gefleckten': 7416,\n",
              " 'gefleckte': 7415,\n",
              " 'gefertigt': 7414,\n",
              " 'gefalteten': 7413,\n",
              " 'gefaltet': 7412,\n",
              " 'knietiefes': 7569,\n",
              " 'geeigneten': 7411,\n",
              " 'gedeckt': 7409,\n",
              " 'gebogen': 7407,\n",
              " 'geblieben': 7406,\n",
              " 'gebeugter': 7404,\n",
              " 'gart': 7401,\n",
              " 'fügt': 7400,\n",
              " 'förmlich': 7399,\n",
              " 'frühen': 7398,\n",
              " 'frisst': 7397,\n",
              " 'freundlich': 7396,\n",
              " 'frequentierten': 7395,\n",
              " 'freies': 7393,\n",
              " 'freie': 7392,\n",
              " 'formiert': 7390,\n",
              " 'formelle': 7389,\n",
              " 'flüstert': 7388,\n",
              " 'flüstern': 7387,\n",
              " 'fließenden': 7385,\n",
              " 'handgemachten': 7486,\n",
              " 'fliederfarbener': 7383,\n",
              " 'fliederfarbenen': 7382,\n",
              " 'flickt': 7381,\n",
              " 'spinnt': 7799,\n",
              " 'flattert': 7378,\n",
              " 'flattern': 7377,\n",
              " 'flankiert': 7376,\n",
              " 'filmen': 7374,\n",
              " 'festlich': 7371,\n",
              " 'überladenen': 8001,\n",
              " 'festgemacht': 7370,\n",
              " 'festgehalten': 7369,\n",
              " 'fertiggestellten': 7368,\n",
              " 'ferngesteuerten': 7367,\n",
              " 'feil': 7366,\n",
              " 'fehlt': 7364,\n",
              " 'faltet': 7361,\n",
              " 'falsche': 7360,\n",
              " 'europäischer': 7358,\n",
              " 'europäischen': 7357,\n",
              " 'ethnischer': 7356,\n",
              " 'erörtert': 7354,\n",
              " 'erzeugt': 7353,\n",
              " 'erwachsenes': 7352,\n",
              " 'erschöpft': 7350,\n",
              " 'erledigt': 7345,\n",
              " 'erfrischenden': 7337,\n",
              " 'erforschen': 7336,\n",
              " 'entsorgt': 7333,\n",
              " 'entsetzt': 7332,\n",
              " 'entschlossen': 7331,\n",
              " 'entleert': 7329,\n",
              " 'entlangfahren': 7328,\n",
              " 'englische': 7326,\n",
              " 'em': 7325,\n",
              " 'elektronisches': 7324,\n",
              " 'elektrische': 7323,\n",
              " 'eleganten': 7322,\n",
              " 'ekstatisch': 7321,\n",
              " 'einhändigen': 7316,\n",
              " 'irgendeinen': 7542,\n",
              " 'eingepackt': 7314,\n",
              " 'eifrig': 7307,\n",
              " 'ebenso': 7306,\n",
              " 'e': 7305,\n",
              " 'dünnes': 7304,\n",
              " 'döst': 7303,\n",
              " 'durchtrennt': 7301,\n",
              " 'durchsuchen': 7300,\n",
              " 'durchsichtigen': 7299,\n",
              " 'durchsichtige': 7298,\n",
              " 'durchnässt': 7295,\n",
              " 'dreckige': 7287,\n",
              " 'ländlicher': 7623,\n",
              " 'dominiert': 7284,\n",
              " 'diversen': 7283,\n",
              " 'dient': 7281,\n",
              " 'dickem': 7280,\n",
              " 'denselben': 7278,\n",
              " 'dargestellt': 7273,\n",
              " 'dar': 7272,\n",
              " 'dann': 7271,\n",
              " 'danebenstehen': 7270,\n",
              " 'coolen': 7269,\n",
              " 'organisieren': 7681,\n",
              " 'cool': 7268,\n",
              " 'chirurgischen': 7267,\n",
              " 'bäuchlings': 7266,\n",
              " 'bäckt': 7265,\n",
              " 'burgunderfarbener': 7264,\n",
              " 'bummelt': 7263,\n",
              " 'buddeln': 7262,\n",
              " 'buckelt': 7261,\n",
              " 'brusttief': 7258,\n",
              " 'brennen': 7255,\n",
              " 'breite': 7254,\n",
              " 'braunschwarzer': 7253,\n",
              " 'braun-schwarz-weißer': 7251,\n",
              " 'boxt': 7249,\n",
              " 'bockendes': 7247,\n",
              " 'blutender': 7245,\n",
              " 'blau-weißem': 7242,\n",
              " 'blau-weiße': 7241,\n",
              " 'blau-gelber': 7240,\n",
              " 'bisschen': 7239,\n",
              " 'bin': 7238,\n",
              " 'regenbogenfarbenen': 7715,\n",
              " 'bewölkter': 7237,\n",
              " 'beworfen': 7236,\n",
              " 'riecht': 7724,\n",
              " 'bewegungslos': 7234,\n",
              " 'bewaffneten': 7233,\n",
              " 'bewachsenen': 7232,\n",
              " 'bevölkerten': 7231,\n",
              " 'betrieben': 7229,\n",
              " 'betreiben': 7228,\n",
              " 'betonierten': 7227,\n",
              " 'beteiligen': 7226,\n",
              " 'bestimmten': 7225,\n",
              " 'besten': 7223,\n",
              " 'bestaunen': 7222,\n",
              " 'besseren': 7221,\n",
              " 'besprüht': 7220,\n",
              " 'besonders': 7218,\n",
              " 'besonderen': 7217,\n",
              " 'besichtigen': 7216,\n",
              " 'demselben': 7277,\n",
              " 'besetztes': 7215,\n",
              " 'beseitigen': 7214,\n",
              " 'beschädigten': 7213,\n",
              " 'beschreibt': 7212,\n",
              " 'beschneidet': 7211,\n",
              " 'beschmierte': 7210,\n",
              " 'beschließen': 7209,\n",
              " 'berühmten': 7208,\n",
              " 'berät': 7207,\n",
              " 'bernsteinfarbener': 7206,\n",
              " 'berittener': 7205,\n",
              " 'bergauf': 7204,\n",
              " 'bemaltem': 7202,\n",
              " 'beleuchtetes': 7200,\n",
              " 'belegten': 7198,\n",
              " 'beladenes': 7197,\n",
              " 'bekleideten': 7195,\n",
              " 'beinhaltet': 7194,\n",
              " 'behandelt': 7187,\n",
              " 'beginnender': 7186,\n",
              " 'begeistertes': 7185,\n",
              " 'befördern': 7184,\n",
              " 'befreien': 7183,\n",
              " 'beeindruckenden': 7181,\n",
              " 'beeindrucken': 7180,\n",
              " 'bedruckter': 7179,\n",
              " 'bearbeiten': 7177,\n",
              " 'baumbestandenen': 7172,\n",
              " 'auszuprobieren': 7170,\n",
              " 'aussteigen': 7169,\n",
              " 'auspackt': 7167,\n",
              " 'ausgetragen': 7166,\n",
              " 'ausgestreckter': 7165,\n",
              " 'aufzuschlagen': 7162,\n",
              " 'aufzuführen': 7161,\n",
              " 'aufzufallen': 7160,\n",
              " 'aufträgt': 7157,\n",
              " 'aufgespannt': 7154,\n",
              " 'aufgesetztem': 7152,\n",
              " 'aufbäumenden': 7149,\n",
              " 'arrangiert': 7146,\n",
              " 'arbeitender': 7142,\n",
              " 'anzuzünden': 7139,\n",
              " 'anstatt': 7136,\n",
              " 'anschneiden': 7135,\n",
              " 'anhören': 7134,\n",
              " 'angestrengt': 7132,\n",
              " 'angestarrt': 7131,\n",
              " 'angeseilt': 7130,\n",
              " 'angeleuchtet': 7129,\n",
              " 'Überdachung': 7989,\n",
              " 'angelegt': 7128,\n",
              " 'angehört': 7127,\n",
              " 'ringsum': 7725,\n",
              " 'angefahren': 7125,\n",
              " 'amp': 7124,\n",
              " 'amische': 7123,\n",
              " 'alles': 7122,\n",
              " 'aktiv': 7120,\n",
              " 'aggressiv': 7119,\n",
              " 'achten': 7117,\n",
              " 'abprallen': 7113,\n",
              " 'abgelenkt': 7110,\n",
              " 'abgeleckt': 7109,\n",
              " 'abgelassen': 7108,\n",
              " 'abgefeuert': 7107,\n",
              " 'abfallenden': 7106,\n",
              " 'Zügel': 7104,\n",
              " 'Zweirad': 7101,\n",
              " 'Zustand': 7099,\n",
              " 'Zusammensein': 7097,\n",
              " 'Zuneigung': 7096,\n",
              " 'Zuhörerschaft': 7094,\n",
              " 'Zuhörern': 7093,\n",
              " 'Zugfahrt': 7090,\n",
              " 'Zirkus': 7086,\n",
              " 'Zielort': 7085,\n",
              " 'Zielfernrohr': 7084,\n",
              " 'Zickzack': 7082,\n",
              " 'Zentimeter': 7081,\n",
              " 'selbstgebastelten': 7779,\n",
              " 'Zementlaster': 7080,\n",
              " 'Zeltplatz': 7079,\n",
              " 'Zwillinge': 7102,\n",
              " 'Zaubertrick': 7076,\n",
              " 'Zahnstocher': 7073,\n",
              " 'Zahlen': 7071,\n",
              " 'Yogaübungen': 7070,\n",
              " 'Yankees': 7069,\n",
              " 'Yamaha': 7068,\n",
              " 'Würste': 7067,\n",
              " 'Würfel': 7066,\n",
              " 'Wälder': 7065,\n",
              " 'Wrestler': 7063,\n",
              " 'Wohnhauses': 7058,\n",
              " 'Winterbekleidung': 7057,\n",
              " 'hellhäutiges': 7496,\n",
              " 'flussabwärts': 7386,\n",
              " 'Wildwasserrafting': 7056,\n",
              " 'Wii': 7055,\n",
              " 'Whirlpool': 7054,\n",
              " 'Wettschwimmen': 7053,\n",
              " 'Westernstil': 7052,\n",
              " 'Werkzeuggürteln': 7051,\n",
              " 'Werkbank': 7050,\n",
              " 'Wells': 7047,\n",
              " 'Weise': 7045,\n",
              " 'Weinflasche': 7044,\n",
              " 'Weihnachtsmotiv': 7043,\n",
              " 'Weihnachtsmannmützen': 7042,\n",
              " 'Weges': 7039,\n",
              " 'Wegen': 7038,\n",
              " 'Wasserspeier': 7034,\n",
              " 'Wasserskulptur': 7033,\n",
              " 'Wasserschlacht': 7032,\n",
              " 'Wasserrohre': 7031,\n",
              " 'Zugführer': 7091,\n",
              " 'Wasserrand': 7030,\n",
              " 'Wasserlache': 7028,\n",
              " 'Wasserfontänen': 7027,\n",
              " 'Wasserbrunnen': 7026,\n",
              " 'Waschmaschinen': 7025,\n",
              " 'Warnkegel': 7021,\n",
              " 'dehnen': 7276,\n",
              " 'Wandobjekt': 7020,\n",
              " 'Wandbemalung': 7018,\n",
              " 'Walkman': 7017,\n",
              " 'Waldlandschaft': 7016,\n",
              " 'verhaftet': 7881,\n",
              " 'Waldgegend': 7015,\n",
              " 'Waggon': 7011,\n",
              " 'Vorteil': 7009,\n",
              " 'Vorräte': 7007,\n",
              " 'Vorgarten': 7005,\n",
              " 'Vorderpfoten': 7003,\n",
              " 'Vitrine': 7000,\n",
              " 'Videoaufnahme': 6996,\n",
              " 'Verwunderung': 6995,\n",
              " 'Verteidigern': 6994,\n",
              " 'Verkäuferin': 6992,\n",
              " 'Verkehrsschild': 6990,\n",
              " 'Verkehrspolizist': 6989,\n",
              " 'Verkaufstand': 6988,\n",
              " 'Verkaufsautomaten': 6987,\n",
              " 'Vergrößerungsglas': 6986,\n",
              " 'Vegas': 6985,\n",
              " 'Vase': 6984,\n",
              " 'Vader': 6983,\n",
              " 'Urinal': 6981,\n",
              " 'Ureinwohnerin': 6980,\n",
              " 'Unterhaltungskünstler': 6976,\n",
              " 'Unterführung': 6974,\n",
              " 'Unrat': 6972,\n",
              " 'Union': 6971,\n",
              " 'Unfallstelle': 6970,\n",
              " 'Umzugs': 6967,\n",
              " 'Umschlag': 6965,\n",
              " 'träg': 7850,\n",
              " 'U-Bahn-Zug': 6960,\n",
              " 'Tümpel': 6958,\n",
              " 'Tüchern': 6957,\n",
              " 'Töchter': 6955,\n",
              " 'Turnkleidung': 6951,\n",
              " 'Tuba': 6947,\n",
              " 'komischer': 7572,\n",
              " 'Trägershirt': 6946,\n",
              " 'Trägerhemden': 6945,\n",
              " 'Truthähne': 6944,\n",
              " 'Tropfen': 6942,\n",
              " 'Trittsteinen': 6941,\n",
              " 'Trinkgeld': 6940,\n",
              " 'Treffen': 6937,\n",
              " 'Trainingsanzügen': 6932,\n",
              " 'Werferhügel': 7049,\n",
              " 'Tragen': 6931,\n",
              " 'Toyota': 6930,\n",
              " 'Touristin': 6929,\n",
              " 'Torbogens': 6926,\n",
              " 'Topfpflanzen': 6925,\n",
              " 'Tomate': 6924,\n",
              " 'übersäte': 8005,\n",
              " 'Toast': 6922,\n",
              " 'Tischler': 6921,\n",
              " 'Tintenfisch': 6920,\n",
              " 'Tiers': 6918,\n",
              " 'Tiefschnee': 6917,\n",
              " 'lieben': 7613,\n",
              " 'Theaterstück': 6913,\n",
              " 'Text': 6912,\n",
              " 'unbekannte': 7865,\n",
              " 'Telefonzellen': 6910,\n",
              " 'Telefontechniker': 6909,\n",
              " 'Telefonleitung': 6908,\n",
              " 'Teenagerinnen': 6905,\n",
              " 'Teenager-Jungen': 6904,\n",
              " 'Technik': 6901,\n",
              " 'Teamkollege': 6900,\n",
              " 'Taubenschwarm': 6899,\n",
              " 'Tatort': 6898,\n",
              " 'Tasten': 6897,\n",
              " 'Taste': 6896,\n",
              " 'Tarnjacke': 6893,\n",
              " 'Tanzvorführung': 6889,\n",
              " 'Tanzstudio': 6888,\n",
              " 'Tanzende': 6887,\n",
              " 'Tank-Top': 6886,\n",
              " 'Talkshow': 6884,\n",
              " 'vierrädrigen': 7908,\n",
              " 'Takt': 6883,\n",
              " 'Taille': 6882,\n",
              " 'Tackle': 6880,\n",
              " 'Süßigkeitenladen': 6878,\n",
              " 'Söhnen': 6876,\n",
              " 'Säuglings': 6875,\n",
              " 'Säcke': 6874,\n",
              " 'Szenerie': 6873,\n",
              " 'Sweater': 6872,\n",
              " 'Suppe': 6870,\n",
              " 'Sumpf': 6869,\n",
              " 'hellrotes': 7500,\n",
              " 'Stängel': 6864,\n",
              " 'Stände': 6863,\n",
              " 'Stäbe': 6862,\n",
              " 'Studentenwohnheim': 6857,\n",
              " 'Strumpfhosen': 6855,\n",
              " 'Strommasten': 6854,\n",
              " 'Stromleitungen': 6853,\n",
              " 'Streitkräfte': 6851,\n",
              " 'Streikschildern': 6850,\n",
              " 'Streicher': 6848,\n",
              " 'Straßenmarathon': 6845,\n",
              " 'Straßenkünstlern': 6842,\n",
              " 'Straßenhändlers': 6840,\n",
              " 'Straßengeschäft': 6839,\n",
              " 'Strandspaziergang': 6837,\n",
              " 'Strandbuggy': 6835,\n",
              " 'Story': 6834,\n",
              " 'Stoppschild': 6833,\n",
              " 'Stolz': 6831,\n",
              " 'Stofftieren': 6830,\n",
              " 'Stocks': 6828,\n",
              " 'Stirnlampe': 6827,\n",
              " 'Stiften': 6826,\n",
              " 'Steuer': 6825,\n",
              " 'Sternen': 6823,\n",
              " 'Steinhaufen': 6816,\n",
              " 'Steinbrücke': 6815,\n",
              " 'Steiger': 6813,\n",
              " 'Statur': 6812,\n",
              " 'Standbild': 6809,\n",
              " 'Stammes': 6808,\n",
              " 'Stahl': 6806,\n",
              " 'Stadtbus': 6801,\n",
              " 'Stadtbrunnen': 6800,\n",
              " 'Stadions': 6798,\n",
              " 'Sprüngen': 6796,\n",
              " 'Sprinkleranlage': 6793,\n",
              " 'Sportteam': 6792,\n",
              " 'Sportstudio': 6791,\n",
              " 'Sportjacke': 6790,\n",
              " 'Sportgerät': 6788,\n",
              " 'dir': 7282,\n",
              " 'Sportfans': 6786,\n",
              " 'Spinnkurs': 6783,\n",
              " 'Spießen': 6782,\n",
              " 'Spielzeugs': 6780,\n",
              " 'Spielzeuglaster': 6777,\n",
              " 'Spielzeuggitarre': 6774,\n",
              " 'Spielzeugeisenbahn': 6772,\n",
              " 'Spielzeug-Basketballkorb': 6770,\n",
              " 'Spielkarten': 6766,\n",
              " 'Spielhaus': 6765,\n",
              " 'Spielgeräten': 6764,\n",
              " 'Spiele': 6763,\n",
              " 'Spiegelungen': 6762,\n",
              " 'Spiderman': 6761,\n",
              " 'Speiseraum': 6760,\n",
              " 'Speerwurf': 6759,\n",
              " 'Spare': 6756,\n",
              " 'Spachtel': 6755,\n",
              " 'Sonnenuntergangs': 6751,\n",
              " 'Sonnenhüten': 6749,\n",
              " 'Sonnenbad': 6748,\n",
              " 'Sonnenaufgang': 6747,\n",
              " 'Sommerkleidung': 6743,\n",
              " 'Sommerkleider': 6742,\n",
              " 'Soldatinnen': 6741,\n",
              " 'Softdrink': 6740,\n",
              " 'Soda': 6739,\n",
              " 'Skimboarden': 6734,\n",
              " 'Skibrille': 6732,\n",
              " 'Sketch': 6730,\n",
              " 'Skaterpark': 6729,\n",
              " 'Skaterbahn': 6728,\n",
              " 'Skateboard-Kunststück': 6723,\n",
              " 'Sitzender': 6721,\n",
              " 'Sitze': 6720,\n",
              " 'Sitzbank': 6719,\n",
              " 'Situation': 6718,\n",
              " 'Shampoo': 6713,\n",
              " 'Set': 6712,\n",
              " 'Sessellift': 6711,\n",
              " 'Serviette': 6710,\n",
              " 'Seifenwasser': 6702,\n",
              " 'Segeln': 6700,\n",
              " 'Segel': 6699,\n",
              " 'Seahawks': 6698,\n",
              " 'Schülerinnen': 6697,\n",
              " 'laufenden': 7600,\n",
              " 'Schäferhunde': 6694,\n",
              " 'Schwäne': 6693,\n",
              " 'Schwimmring': 6691,\n",
              " 'Schwimmflügel': 6687,\n",
              " 'Schwerarbeiter': 6685,\n",
              " 'Schwelle': 6684,\n",
              " 'Schweißen': 6682,\n",
              " 'zapft': 7957,\n",
              " 'trotzt': 7849,\n",
              " 'Schweinefleisch': 6681,\n",
              " 'Schwarzes': 6680,\n",
              " 'Schwarzen': 6679,\n",
              " 'Schutzhelme': 6678,\n",
              " 'Schuppen': 6676,\n",
              " 'Schulklasse': 6674,\n",
              " 'Thomas': 6915,\n",
              " 'Schulkindern': 6673,\n",
              " 'Schulkantine': 6672,\n",
              " 'Schulen': 6670,\n",
              " 'Schulalter': 6669,\n",
              " 'Sprühdosen': 6795,\n",
              " 'Schubkarren': 6667,\n",
              " 'Schreiben': 6664,\n",
              " 'Schraubenzieher': 6663,\n",
              " 'Tarnanzügen': 6890,\n",
              " 'Schnurrbärten': 6655,\n",
              " 'Schnellimbiss': 6654,\n",
              " 'Sturm': 6860,\n",
              " 'Schnellboot': 6653,\n",
              " 'Schneidersitz': 6652,\n",
              " 'regennassen': 7717,\n",
              " 'Schönheit': 6696,\n",
              " 'Schneiderin': 6651,\n",
              " 'Schneidbrett': 6650,\n",
              " 'Schneeschaufel': 6647,\n",
              " 'Schnees': 6646,\n",
              " 'Schneelandschaft': 6645,\n",
              " 'Schneejacke': 6644,\n",
              " 'Schneebrille': 6643,\n",
              " 'Schneeausrüstung': 6641,\n",
              " 'Schneeanzügen': 6640,\n",
              " 'Schmetterlinge': 6638,\n",
              " 'Schlittschuhe': 6637,\n",
              " 'Schleuder': 6634,\n",
              " 'Schleppe': 6633,\n",
              " 'Schleifmaschine': 6632,\n",
              " 'Schlangen': 6630,\n",
              " 'Schlagmanns': 6629,\n",
              " 'Schlagen': 6628,\n",
              " 'Schlafsäcken': 6627,\n",
              " 'Schlafanzügen': 6626,\n",
              " 'Schichten': 6619,\n",
              " 'Scheune': 6618,\n",
              " 'Scheitelpunkt': 6616,\n",
              " 'Schaulustiger': 6614,\n",
              " 'Schaulustigen': 6613,\n",
              " 'Schaukelpferd': 6612,\n",
              " 'Schaukelgestell': 6611,\n",
              " 'Schar': 6608,\n",
              " 'Schachbrett': 6607,\n",
              " 'Saxophonisten': 6606,\n",
              " 'Sanitäter': 6603,\n",
              " 'Sandsturm': 6602,\n",
              " 'Sandsack': 6601,\n",
              " 'Sandkasten': 6600,\n",
              " 'Röcke': 6595,\n",
              " 'Steinsäulen': 6820,\n",
              " 'Räder': 6594,\n",
              " 'blau-weißes': 7243,\n",
              " 'Rugby-Spiel': 6591,\n",
              " 'Rudermannschaft': 6590,\n",
              " 'Rothaarige': 6589,\n",
              " 'verglasten': 7879,\n",
              " 'Rollschuhbahn': 6586,\n",
              " 'Rodgers': 6583,\n",
              " 'errichteten': 7349,\n",
              " 'Rodeoarena': 6582,\n",
              " 'Rodeo-Clown': 6581,\n",
              " 'Ritualtanz': 6577,\n",
              " 'Ringe': 6574,\n",
              " 'Rinder': 6573,\n",
              " 'Ribbon': 6571,\n",
              " 'Rettungskraft': 6570,\n",
              " 'unbeholfen': 7864,\n",
              " 'Rettungshelfer': 6569,\n",
              " 'kahlem': 7552,\n",
              " 'Rentier': 6564,\n",
              " 'Rennmotorrad': 6563,\n",
              " 'Rennhund': 6562,\n",
              " 'Renaissance-Fest': 6560,\n",
              " 'Reishut': 6556,\n",
              " 'tanzende': 7832,\n",
              " 'Reisfeld': 6555,\n",
              " 'Regierung': 6552,\n",
              " 'Regenkleidung': 6551,\n",
              " 'Regenjacke': 6550,\n",
              " 'Rauchwolke': 6546,\n",
              " 'Rasseln': 6544,\n",
              " 'Rasiermesser': 6543,\n",
              " 'Rasierer': 6542,\n",
              " 'Rasiercreme': 6541,\n",
              " 'Querflöte': 6538,\n",
              " 'Päckchen': 6536,\n",
              " 'Punkte': 6535,\n",
              " 'Pulver': 6534,\n",
              " 'verkehrsreiche': 7884,\n",
              " 'Pullovers': 6533,\n",
              " 'Pulk': 6532,\n",
              " 'Projektorleinwand': 6529,\n",
              " 'Problem': 6528,\n",
              " 'Probe': 6527,\n",
              " 'Presslufthammer': 6525,\n",
              " 'Preisrichter': 6523,\n",
              " 'Preise': 6522,\n",
              " 'Power': 6520,\n",
              " 'Postmitarbeiter': 6519,\n",
              " 'gemustertem': 7435,\n",
              " 'Ponchos': 6515,\n",
              " 'Polo-Hemd': 6514,\n",
              " 'Polizeifahrzeug': 6512,\n",
              " 'Polizeibeamter': 6511,\n",
              " 'Polizeiauto': 6510,\n",
              " 'Plüschtieren': 6509,\n",
              " 'Plattenladen': 6508,\n",
              " 'Plastikwanne': 6507,\n",
              " 'Plastikflasche': 6505,\n",
              " 'Plastikeimer': 6504,\n",
              " 'Plastikbecher': 6503,\n",
              " 'Pinien': 6500,\n",
              " 'Pinguine': 6499,\n",
              " 'Pilz': 6498,\n",
              " 'Piercings': 6497,\n",
              " 'Piano': 6495,\n",
              " 'Pfote': 6493,\n",
              " 'Pferdeschwänzen': 6491,\n",
              " 'Pferch': 6489,\n",
              " 'Pfeiler': 6488,\n",
              " 'Pfauenkostüm': 6487,\n",
              " 'Pfau': 6486,\n",
              " 'Pfannkuchen': 6484,\n",
              " 'Pfadfindern': 6483,\n",
              " 'Personenzug': 6481,\n",
              " 'Perle': 6480,\n",
              " 'Pedale': 6477,\n",
              " 'Pedal': 6476,\n",
              " 'Pavillon': 6475,\n",
              " 'Patient': 6474,\n",
              " 'Passagier': 6473,\n",
              " 'vesammelt': 7905,\n",
              " 'Partnerin': 6472,\n",
              " 'Partner': 6471,\n",
              " 'Parkuhr': 6469,\n",
              " 'Parkgarage': 6468,\n",
              " 'Parken': 6467,\n",
              " 'Paris': 6465,\n",
              " 'Pappteller': 6464,\n",
              " 'Pappschachtel': 6463,\n",
              " 'Papierhut': 6461,\n",
              " 'Papagei': 6460,\n",
              " 'Pain': 6456,\n",
              " 'Outdoor-Bekleidung': 6454,\n",
              " 'Orten': 6453,\n",
              " 'Orioles': 6452,\n",
              " 'Orgel': 6451,\n",
              " 'Ordner': 6450,\n",
              " 'Orchesters': 6449,\n",
              " 'Opfer': 6448,\n",
              " 'Open-Air-Konzert': 6447,\n",
              " 'Open-Air-Festival': 6446,\n",
              " 'Olympische': 6445,\n",
              " 'Ohrschützern': 6443,\n",
              " 'Ohrringen': 6442,\n",
              " 'Objektiv': 6439,\n",
              " 'Oberleitungsfahrzeug': 6438,\n",
              " 'Obama': 6437,\n",
              " 'Nägel': 6434,\n",
              " 'Notre': 6433,\n",
              " 'Notenblättern': 6431,\n",
              " 'Notenblatt': 6430,\n",
              " 'Nonnentracht': 6427,\n",
              " 'Bekleidungsgeschäft': 3192,\n",
              " 'Beim': 3191,\n",
              " 'unterhalten': 186,\n",
              " 'Bauernhof': 3189,\n",
              " 'Wachmann': 2408,\n",
              " 'BMX-Rad': 3177,\n",
              " 'Außenseite': 3176,\n",
              " 'Softballspieler': 4859,\n",
              " 'Auszeichnung': 3175,\n",
              " 'Capris': 5743,\n",
              " 'Ausflug': 3174,\n",
              " 'musiziert': 7653,\n",
              " 'Aquarium': 2787,\n",
              " 'Krügen': 6228,\n",
              " 'Karton': 1973,\n",
              " 'Massen': 3843,\n",
              " 'Ansammlung': 3168,\n",
              " 'verläuft': 5429,\n",
              " 'Angel': 3167,\n",
              " 'free': 7391,\n",
              " 'paar': 303,\n",
              " 'Bulldozer': 3202,\n",
              " 'Afrikanische': 3163,\n",
              " 'Drachen': 1337,\n",
              " 'wählt': 3150,\n",
              " 'Einige': 429,\n",
              " 'wir': 3148,\n",
              " 'wenig': 3146,\n",
              " 'Dosen': 2540,\n",
              " 'Bier': 464,\n",
              " 'Wandgemälde': 7019,\n",
              " 'ärmellosem': 3154,\n",
              " 'Tourbus': 6928,\n",
              " 'leisten': 5293,\n",
              " 'tätowierter': 3131,\n",
              " 'schick': 7746,\n",
              " ' ': 671,\n",
              " 'Schaufensterbummel': 6610,\n",
              " 'trocknet': 3130,\n",
              " 'teilnimmt': 3128,\n",
              " 'künstlichem': 7589,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgN2KkyJ6kf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "26891413-c9ed-4929-ea47-83effa73bb26"
      },
      "source": [
        "print(f\"학습 데이터셋(training dataset) 크기: {len(train_dataset.examples)}개\")\n",
        "print(f\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset.examples)}개\")\n",
        "print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset.examples)}개\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a74dc9945a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"학습 데이터셋(training dataset) 크기: {len(train_dataset.examples)}개\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset.examples)}개\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset.examples)}개\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/datapipe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'{0}' object has no attribute '{1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ShardingFilterIterDataPipe' object has no attribute 'examples"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6clmgtf7ENF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "69825125-a0ae-434f-e527-4a91587f871f"
      },
      "source": [
        "# 학습 데이터 중 하나를 선택해 출력\n",
        "print(vars(train_dataset.examples[30])['src'])\n",
        "print(vars(train_dataset.examples[30])['trg'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-78bf2bb27678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 학습 데이터 중 하나를 선택해 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/datapipe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'{0}' object has no attribute '{1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ShardingFilterIterDataPipe' object has no attribute 'examples"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvPZWqQk9Ea1"
      },
      "source": [
        "* **필드(field)** 객체의 **build_vocab** 메서드를 이용해 영어와 독어의 단어 사전을 생성합니다.\n",
        "  * **최소 2번 이상** 등장한 단어만을 선택합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR7DqpLU8hiT"
      },
      "source": [
        "SRC.build_vocab(train_dataset, min_freq=2)\n",
        "TRG.build_vocab(train_dataset, min_freq=2)\n",
        "\n",
        "print(f\"len(SRC): {len(SRC.vocab)}\")\n",
        "print(f\"len(TRG): {len(TRG.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4p3feo19aDP"
      },
      "source": [
        "print(TRG.vocab.stoi[\"abcabc\"]) # 없는 단어: 0\n",
        "print(TRG.vocab.stoi[TRG.pad_token]) # 패딩(padding): 1\n",
        "print(TRG.vocab.stoi[\"<sos>\"]) # <sos>: 2\n",
        "print(TRG.vocab.stoi[\"<eos>\"]) # <eos>: 3\n",
        "print(TRG.vocab.stoi[\"hello\"])\n",
        "print(TRG.vocab.stoi[\"world\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4PiQ3HR9WKP"
      },
      "source": [
        "* 한 문장에 포함된 단어가 연속적으로 **LSTM**에 입력되어야 합니다.\n",
        "    * 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋습니다.\n",
        "    * 이를 위해 BucketIterator를 사용합니다.\n",
        "    * **배치 크기(batch size)**: 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM6EVV2t9BHd"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# 일반적인 데이터 로더(data loader)의 iterator와 유사하게 사용 가능\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset, test_dataset),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3v8n8XC-snA"
      },
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    print(f\"첫 번째 배치 크기: {src.shape}\")\n",
        "\n",
        "    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
        "    for i in range(src.shape[0]):\n",
        "        print(f\"인덱스 {i}: {src[i][0].item()}\")\n",
        "\n",
        "    # 첫 번째 배치만 확인\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oguDx24e-3Db"
      },
      "source": [
        "#### **인코더(Encoder) 아키텍처**\n",
        "\n",
        "* 주어진 소스 문장을 **문맥 벡터(context vector)로 인코딩**합니다.\n",
        "* LSTM은 hidden state과 cell state을 반환합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원핫 인코딩 차원\n",
        "    * **embed_dim**: 임베딩(embedding) 차원\n",
        "    * **hidden_dim**: 히든 상태(hidden state) 차원\n",
        "    * **n_layers**: RNN 레이어의 개수\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac28d5DL_ceY"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 인코더(Encoder) 아키텍처 정의\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding)을 특정 차원의 임베딩으로 매핑하는 레이어\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n",
        "        \n",
        "        # 드롭아웃(dropout)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 인코더는 소스 문장을 입력으로 받아 문맥 벡터(context vector)를 반환        \n",
        "    def forward(self, src):\n",
        "        # src: [단어 개수, 배치 크기]: 각 단어의 인덱스(index) 정보\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs: [단어 개수, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "        # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "\n",
        "        # 문맥 벡터(context vector) 반환\n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pHj4IlvBzPe"
      },
      "source": [
        "#### **디코더(Decoder) 아키텍처**\n",
        "\n",
        "* 주어진 문맥 벡터(context vector)를 **타겟 문장으로 디코딩**합니다.\n",
        "* LSTM은 hidden state과 cell state을 반환합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원핫 인코딩 차원\n",
        "    * **embed_dim**: 임베딩(embedding) 차원\n",
        "    * **hidden_dim**: 히든 상태(hidden state) 차원\n",
        "    * **n_layers**: RNN 레이어의 개수\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fusf6_9yDmfM"
      },
      "source": [
        "# 디코더(Decoder) 아키텍처 정의\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding) 말고 특정 차원의 임베딩으로 매핑하는 레이어\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n",
        "        \n",
        "        # FC 레이어 (인코더와 구조적으로 다른 부분)\n",
        "        self.output_dim = output_dim\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        # 드롭아웃(dropout)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환     \n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input: [배치 크기]: 단어의 개수는 항상 1개이도록 구현\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]\n",
        "        # cell = context: [레이어 개수, 배치 크기, 히든 차원]\n",
        "        input = input.unsqueeze(0)\n",
        "        # input: [단어 개수 = 1, 배치 크기]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # output: [단어 개수 = 1, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "        # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "\n",
        "        # 단어 개수는 어차피 1개이므로 차원 제거\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [배치 크기, 출력 차원]\n",
        "        \n",
        "        # (현재 출력 단어, 현재까지의 모든 단어의 정보, 현재까지의 모든 단어의 정보)\n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j17WxtpJI_9V"
      },
      "source": [
        "#### **Seq2Seq 아키텍처**\n",
        "\n",
        "* 앞서 정의한 인코더(encoder)와 디코더(decoder)를 가지고 있는 하나의 아키텍처입니다.\n",
        "    * **인코더(encoder)**: 주어진 소스 문장을 문맥 벡터(context vector)로 인코딩합니다.\n",
        "    * **디코더(decoder)**: 주어진 문맥 벡터(context vector)를 타겟 문장으로 디코딩합니다.\n",
        "    * 단, **디코더는 한 단어씩** 넣어서 한 번씩 결과를 구합니다.\n",
        "* **Teacher forcing**: 디코더의 예측(prediction)을 다음 입력으로 사용하지 않고, 실제 목표 출력(ground-truth)을 다음 입력으로 사용하는 기법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsA6C6B5Glhc"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    # 학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio를 넣기\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [단어 개수, 배치 크기]\n",
        "        # trg: [단어 개수, 배치 크기]\n",
        "        # 먼저 인코더를 거쳐 문맥 벡터(context vector)를 추출\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # 디코더(decoder)의 최종 결과를 담을 텐서 객체 만들기\n",
        "        trg_len = trg.shape[0] # 단어 개수\n",
        "        batch_size = trg.shape[1] # 배치 크기\n",
        "        trg_vocab_size = self.decoder.output_dim # 출력 차원\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # 첫 번째 입력은 항상 <sos> 토큰\n",
        "        input = trg[0, :]\n",
        "\n",
        "        # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보\n",
        "            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n",
        "\n",
        "            # teacher_forcing_ratio: 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyXRWyDrHYSB"
      },
      "source": [
        "#### **학습(Training)**\n",
        "\n",
        "* 하이퍼 파라미터 설정 및 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVsGIVvzMZ-N"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENCODER_EMBED_DIM = 256\n",
        "DECODER_EMBED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT_RATIO = 0.5\n",
        "DEC_DROPOUT_RATIO = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WM3urPiIE1T"
      },
      "source": [
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n",
        "dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n",
        "\n",
        "# Seq2Seq 객체 선언\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcnWnXGIMwHk"
      },
      "source": [
        "* 논문의 내용대로 $\\mathcal{U}(-0.08, 0.08)$의 값으로 **모델 가중치 파라미터 초기화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdfqma4uMaoI"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUwj0UgIS6E"
      },
      "source": [
        "* 학습 및 평가 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeqqI7xfM71V"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXo7ZOclNG2-"
      },
      "source": [
        "# 모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "        # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        # 출력 단어의 인덱스 0은 사용하지 않음\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
        "        trg = trg[1:].view(-1)\n",
        "        # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
        "        \n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "        \n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "        \n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzR8hm9HQ1gb"
      },
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            # 평가할 때 teacher forcing는 사용하지 않음\n",
        "            output = model(src, trg, 0)\n",
        "            # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            # 출력 단어의 인덱스 0은 사용하지 않음\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
        "            trg = trg[1:].view(-1)\n",
        "            # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76X7pR1cLtAl"
      },
      "source": [
        "* 학습(training) 및 검증(validation) 진행\n",
        "    * **학습 횟수(epoch)**: 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVGWe9VtSwx0"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMMkAnGeSyMW"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'seq2seq.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvYlZ85ZRUya"
      },
      "source": [
        "# 학습된 모델 저장\n",
        "from google.colab import files\n",
        "\n",
        "files.download('seq2seq.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6d2isoPL_P0"
      },
      "source": [
        "#### **모델 최종 테스트(testing) 결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfs5GYSpUGeU"
      },
      "source": [
        "!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/ERgwTMYWR7FMhApROaNvZREBTjEDi00ttSzt8ZNj1PS_5g?download=1 -O seq2seq.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co3YMQ2NS0Ia"
      },
      "source": [
        "model.load_state_dict(torch.load('seq2seq.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMvcdNR7YqAo"
      },
      "source": [
        "#### **나만의 데이터로 모델 사용해보기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjHJkeznS1oS"
      },
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    print(f\"전체 소스 토큰: {tokens}\")\n",
        "\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    print(f\"소스 문장 인덱스: {src_indexes}\")\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "        # <eos>를 만나는 순간 끝\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
        "    return trg_tokens[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OSgbkh0Vkq7"
      },
      "source": [
        "example_idx = 10\n",
        "\n",
        "src = vars(test_dataset.examples[example_idx])['src']\n",
        "trg = vars(test_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(f'타겟 문장: {trg}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR4mSubPXOJV"
      },
      "source": [
        "src = tokenize_de(\"Guten Abend.\")\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}